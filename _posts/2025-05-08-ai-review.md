---
layout: post
author: RWG
title: I Ain't Gonna Work on ChatGPT's Farm No More
tag:
    - goal 1
comments: 
  show: true
  fedihost: aoir.social
  fediusername: rwg
  fediid:
---

I think I'm going to get a reputation -- if I don't have it already -- of being one of _those_ people. Those people who really don't like generative AI.

<!-- more -->

My latest ponderings about the problems of generative AI center on academic publishing -- specifically, performing service as a peer reviewer. By my count, over the past 15 years, I've reviewed 150 items -- journal article submissions, conference papers, and book manuscripts. That's an average of 10 a year.

Until recently, my biggest worry about this process was the fact that I am doing free labor that [benefits highly profitable corporations](https://tidsskriftet.no/en/2020/08/kronikk/money-behind-academic-publishing). To mitigate this, I've tried hard to do more peer reviews for genuinely nonprofit organizations, rather than the SAGEs and Taylor Francises of the world. But then again, I often do reviews for journals owned by those companies, so I am also very supportive of calls to [pay for peer reviews](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(21\)02804-X/fulltext).

Now, however, I'm adding a new concern: that I'm not always reviewing the arguments, ideas, and thoughts of my fellow academics -- some of the time I'm reviewing the outputs of machines.

## Improving the Methods Section Generated by ChatGPT
I'm talking of course of generative AI. ChatGPT, which has largely kicked of this craze, is already being used to generate text that is making it into academic journals. _Retraction Watch_ [has found over 100 articles](https://retractionwatch.com/papers-and-peer-reviews-with-evidence-of-chatgpt-writing/) with material obviously generated by that system. 

That sounds like a lot. But David Crotty of _Scholarly Kitchen_ [is tamping down a bit on the panic](https://scholarlykitchen.sspnet.org/2024/03/20/the-latest-crisis-is-the-research-literature-overrun-with-chatgpt-and-llm-generated-articles/), doing some analysis in 2024 and finding that only a tiny percentage of articles have material generated by ChatGPT, and even those are _about_ ChatGPT. (Indeed, [I've written something that quotes ChatGPT purposely](https://www.degruyterbrill.com/document/doi/10.1515/9783110792270-007/html) to make a point about generative AI.) But even so, he concludes by stating, "thereâ€™s no room for complacency" in relation to generative AI language being published in academic journals, "and research integrity vigilance will only become more and more demanding."

I agree. LLMs are a new phenomenon, and academic publishing is slow. I imagine that the number of submitted articles that include LLMs will only increase. And it will be less obvious than the 100 clumsy examples _Retraction Watch_ found -- LLM-generated academic articles will be nearly indistiguishable from human-written papers. 

That means peer reviewers like me will increasingly have to read, analyze, and comment upon computer-generated ideas. I might be asked to read a ChatGPT-generated methods section and critically consider if... what? If ChatGPT's methods are sound? If the researchers' methods are sound? Whose work am I analyzing?

I think that's a major problem.

## Existing Policies Sound Good Until They Don't
It seems that publishers are thinking about this, and there are policies in place. But the problem I see is that the positions of publishers are contradictory. 

Take for example [_New Media & Society_](https://journals.sagepub.com/home/nms) (NMS), a journal that has published my work and counts me as a member of the editorial board (which means I do a _ton_ of reviews for them). 

NMS is a member of the Committee on Publication Ethics (COPE). COPE [has a policy on generative AI](https://publicationethics.org/guidance/cope-position/authorship-and-ai-tools). This [policy states](https://publicationethics.org/guidance/cope-position/authorship-and-ai-tools) that authors
> who use AI tools in the writing of a manuscript, production of images or graphical elements of the paper, or in the collection and analysis of data, must be transparent in disclosing in the Materials and Methods (or similar section) of the paper how the AI tool was used and which tool was used. Authors are fully responsible for the content of their manuscript, even those parts produced by an AI tool, and are thus liable for any breach of publication ethics.

I can get with this. If authors who use generative AI disclose what corporate entity's tool they used and how they used it, I can make a decision about doing the work of reviewing the article.

However, _New Media & Society_ is published by SAGE. In its [policy on AI](https://us.sagepub.com/en-us/nam/artificial-intelligence-policy), SAGE itself echoes COPE in stating
> The use of AI tools that can produce content such as generating references, text, images or any other form of content must be disclosed when used by authors or reviewers.

It also says authors who use generative AI should 
> clearly indicate the use of language models in the manuscript, including which model was used and for what purpose. Please use the methods or acknowledgements section, as appropriate.

So far, so good.

But SAGE also carves out a space for "assistive AI," stating,
> AI tools that make suggestions to improve or enhance your own work, such as tools to improve language, grammar or structure, are considered assistive AI tools and do not require disclosure by authors or reviewers. 

Man, that's a _thin_ line. So, authors don't have to say they used generative AI to improve their language, but they do have to disclose if they used it for generating text? What's the distinction here?

In fields like mine, _words matter._ Think of all the digital ink spilled about terms like "platforms" or "networks" or "influencers" or "surveillance." Think of all the adjectives applied to them over the years -- _generative, lateral, sharing, fast, micro, ubiquitous,_ etc, etc. Then think of ways to define these terms, operationalize them as concepts to analyze social practices, and present one's findings. It's language all the way down. "Improve" (which is to say, "alter") any bit of that with an assistive device and the meanings change -- sometimes subtly, sometimes overtly, often profoundly. 

So the "help with language but don't generate text" distinction doesn't really work for me. And this is not to mention "structure" -- I read that and think about the overall organization and coherence of the article, which is actually an area many of my reviews end up addressing.

I would say that, if an author uses a generative AI tool, even for "language help," I ought to know about it before I review. Disclose if you used generative AI for editing and grammar. Oh, and tell me what corporation's product you're using. I deserve to know.

## Whom Am I Reviewing? And for Whom Am I Working?
That's my main point: I want to review the ideas of fellow humans, not the outputs of machines. Therefore, I need to know from journals and authors whether or not generative AI was used at _any_ point in the process of writing the article. Then I can make a decision. Otherwise, I think it's inevitable that I will be doing the work of improving the output of a machine rather than offering guidance and ideas to a fellow researcher. That cheapens the whole process.

And beyond the insult of having to improve the logic of machines instead of helping my academic peers, I also am concerned that my reviews -- which take hours of work to write -- will be fed into machines without my consent. This could be done by the authors, who want an LLM to help them figure out how to revise based on my review. I'm assuming any authors who use LLMs to write their articles would do this, so, again, I want to know.

Or it could be done by the press, which is interested in making a review machine.

On that last note, I can't help but think that this explains the language/text distinction SAGE is proposing in their policy. It may well reflect a [desire to sell the text of its publications to generative AI firms](https://www.thebookseller.com/news/sage-confirms-it-is-in-talks-to-license-content-to-ai-firms). Their AI policy is quite celebratory:
> Sage recognises the value of artificial intelligence (AI) and its potential to help authors in the research and writing process. Sage welcomes developments in this area to enhance opportunities for generating ideas, accelerating research discovery, synthesising, or analysing findings, polishing language, or structuring a submission.

No doubt these deals might result in "assistive technologies" that are marketed to authors, "tools" that can help improve a text but (so it is claimed) won't do the thinking for the author, peer reviewer assistance, and so on. 

There are already [services that claim to use AI](https://consensus.app) to find sources and generate text. [Adobe offers](https://helpx.adobe.com/ca/acrobat/using/ai-generated-summaries.html) to generate a summary based on an article. Jenni.ai offers ["AI autocomplete"](https://jenni.ai/) to "beat writer's block."

Again: where's the line? At some point "assistance" becomes "generation." Or rather, maybe everything becomes mere "assistance." 

And then peer reviewers like me have to review it.

## Postscript, Since I Know You're Thinking It
Oh, and in case you're wondering: no, I will not use generative AI to assist me with my reviews.
