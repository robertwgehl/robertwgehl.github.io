I"°%<p>Thereâ€™s a new paper out about Mastodon! But unfortunately, itâ€™s a deeply problematic one.</p>

<p>Nobre et alâ€™s â€œMore of the Same? A Study of Images Shared on Mastodonâ€™s Federated Timelineâ€ is a <a href="10.1007/978-3-031-19097-1_11">paper that is now published</a> in proceedings from <em>International Conference on Social Informatics.</em> (Unfortunately, itâ€™s not open access.)</p>

<p>Because Iâ€™m currently researching the fediverse and blogging about that process, I thought Iâ€™d write up notes on this paper.</p>

<p>Why this paper? Frankly, because Iâ€™m pretty certain it violates the community norms, as well as terms of service, of many Mastodon instances. It instantly reminded me of the controversial paper from Zignani et al, <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/3262/3130">â€œMastodon Content Warnings: Inappropriate Contents on a Microblogging Platformâ€</a>, which resulted in a <a href="https://www.sunclipse.org/wp-content/downloads/2020/01/open-letter.html">scathing open letter</a> and the retraction of a dataset from the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/R1HKVS">Harvard Dataverse</a>.</p>

<p>Nobre et alâ€™s â€œMore of the Sameâ€ is a study of image-sharing. The authors claim that it is about image-sharing on Mastodon, but really their focus is on images they culled from Mastodon.socialâ€™s federated timeline. They pulled 4M posts from 103K active users, of which 1M had images. Since they pulled posts from Mastodon.socialâ€™s federated timeline, they saw posts from 4K separate instances. The authors state that a â€œrelevant numberâ€ of the images they found are â€œexplicit.â€ They categorize the images as such after running them through Googleâ€™s <a href="https://cloud.google.com/vision">Vision AI</a> Safe Search system. They also run the images they find through Googleâ€™s image search to trace where the images came from and how they are shared on Mastodon. Ultimately, the authors donâ€™t really make an argument, other than stating in passing that Mastodon needs better moderation, since people share explicit images.</p>

<p>In some ways, â€œMore of the Sameâ€ lives up to its title: itâ€™s more of the same poor scholarship that can be seen in Zignani et al (in fact, Nobre et al cite that controversial paper). Here are my critiques:</p>

<!-- more -->

<h2 id="key-problems-in-more-of-the-same">Key Problems in â€œMore of the Sameâ€</h2>
<h3 id="violations-of-community-standards-and-privacy-policies">Violations of Community Standards and Privacy Policies</h3>
<p>This is the big one. There has been a longstanding community prohibition against taking Mastodon posts out of context. This is something I have observed again and again as I do my participant observation in the fediverse. While many Mastodon users might be ok with having their posts taken, many are not. And many instances prohibit the practice in their terms of service. I myself am very careful about getting explicit consent from anyone I want to quote.</p>

<p>Did the authors examine the privacy policies or terms of service for all â€“ or even any â€“ of the 4K instances they culled content from? Thereâ€™s no indication that they did. Indeed, unlike Zignani et al, these authors donâ€™t even discuss privacy. The assumption seems to be that these posts are â€œpublicâ€ and are thus fair game. But as the <a href="https://aoir.org/reports/ethics3.pdf">Association of Internet Researcherâ€™s ethics guidelines</a> have noted over the years, sometimes â€œpublicâ€ doesnâ€™t mean â€œpublicâ€ â€“ that is, it doesnâ€™t mean â€œI want everyone in the world to see this.â€</p>

<p>And most egregious is the fact that there is no human subjects/institutional review board information in the paper â€“ it doesnâ€™t even appear that the authors took that basic step.</p>

<h3 id="who-consented-to-be-analyzed-by-google">Who Consented to be Analyzed by Google?</h3>
<p>If thereâ€™s no indication that people consented to having their posts scraped, thereâ€™s <strong>definitely</strong> no indication that they consented to have their posts submitted to Google. But the authors do exactly that â€“ twice. They ran images through Googleâ€™s Vision AI and also through Googleâ€™s image search.</p>

<p>How many of the 4K instances seek to avoid being indexed by Google? Iâ€™m actually not sure, but I have seen people discuss using robots.txt and other means to avoid having â€œpublicâ€ posts be indexed. Regardless, exporting content from Mastodon into Googleâ€™s indexes violates community norms, if not terms of service for many instances.</p>

<h3 id="what-is-explicit-and-in-what-context">What is â€œexplicit?â€ And in what context?</h3>
<p>The authors run images through Google safe search and find that a â€œrelevant numberâ€ of them â€œmay be explicitâ€ â€“ that they might have nudity, or violence, or are spoof images. The assumption here is that Google is the arbitor of explicitness online. Maybe Google is, maybe it isnâ€™t, but one key aspect is missing: context. Are they porn images? Are they medical images? What context are they embedded in? Much like the â€œpublicâ€ aspect discussed above, I have observed over and over again Fediverse users saying that they donâ€™t want their posts taken out of context.</p>

<p>In addition, the authors note a number of the images are â€œspoofâ€ images, per the Google analysis, and that spoof images are explicit. How so? Indeed, what does â€œspoofâ€ even mean?</p>

<p>And, none of this accounts for the fact that Mastodon (and other fediverse sites) use content warnings and other devices to allow users a bit of control over what they see.</p>

<p>As I mentioned above, the observation that there are explicit images led the authors to make a rather weak, in-passing argument â€œmoderation is necessary in the Mastodon environment as images, while being a popular media format, indeed display a consistent level of explicit contentâ€ (page 189). But again, without context, these authors cannot really judge whether or not the â€œMastodon environmentâ€ is moderated or not.</p>

<h3 id="a-top-ten-image-sharing-mastodon-instance-is-actually-a-pleroma-instance">A top-ten image sharing Mastodon instance is actually a Pleroma instance</h3>

<p>The authors donâ€™t publish a list of all the 4K instances they scraped content from, but they do post the Top Ten image-sharing instances. Given that they scrape Mastodon.socialâ€™s federated timeline, itâ€™s no surprise that that instance is #1. There are also other, unsurprising inclusions on the top ten: botsin.space (#2), mastodon.online (#6).</p>

<p>Thereâ€™s a surprising #3, though: milker.cafe. The authors talk about milker.cafe at length, because they note that milker.cafeâ€™s 11 active users contributed a whopping 26K images to the dataset.</p>

<p>The problem is milker.cafe is a Pleroma instance, not a Mastodon one. Since they make claims about Mastodon not being moderated enough, this is a bit of a problem. I hadnâ€™t heard of milker.cafe, so I took a look and got â€“ surprise, surprise â€“ eyeballs full of busty waifusâ€¦ once I clicked through the â€œAdults Onlyâ€ warnings. How many of Nobre et alâ€™s â€œexplicitâ€ instances came from milker.cafe? And why didnâ€™t the authors flag this Top Ten image sharing site as a Pleroma instance, not a Mastodon one? Could they not tell the difference?</p>

<h3 id="the-denominator-problem">The Denominator Problem</h3>
<p>Finally, the authors note that the largest instances share the most â€œexplicitâ€ images. This raises the denominator problem (as Eric Jardine might put it): they donâ€™t offer any denominator, so we have no idea what percentage weâ€™re dealing with.</p>

<p>â€¦
As a whole, the paper, in my opinion, violates Mastodon community norms without really contributing much of anything to science. The authorsâ€™ key findings are a) people share images on Mastodon; that b) many of those images come from outside of Mastodon (e.g., Twitter, Reddit); and c) some of those images are â€“ in the judgement of Googleâ€™s Safe Search algorithm â€“ explicit. Therefore, they argue, Mastodon needs moderation. As I see it, the authors are relying on an age-old trick to get published: gin up a moral panic about some new digital media practice.</p>

<p>I  donâ€™t think that these simplistic findings required such a breach of community norms and privacy policies.</p>

<h2 id="what-to-do">What to do?</h2>
<p>The question is: beyond my critiquing this paper, what do we do? I mentioned this paper on scholar.social and people have floated the idea of another open letter, similar to the <a href="https://www.sunclipse.org/wp-content/downloads/2020/01/open-letter.html">one sent about Zignani et al</a>.</p>

<p>But would it be effective? I took a look at the previous open letter and note that many of the remedies requested havenâ€™t been achieved. While Harvardâ€™s Dataverse did pull the Zignani et al dataset, the paper itself remains published and unretracted. Unlike the case of Zignani et al, Nobre et al appear to have not posted their dataset anywhere.</p>

<p>Unlike Zignani et al, however, Nobre et al donâ€™t mention privacy considerations. It could be that an open letter to their institutions (Universidade Federal de Minas Gerais and Universidade Federal de Ouro Preto) <em>might</em> result in these institutionâ€™s ethics boards to take a look at the work of Nobre et al. And since Nobre et al discuss future research plans involving media scraping from Mastodon, an open letter might prompt these researchers to do better.</p>
:ET