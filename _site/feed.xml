<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-25T12:11:06-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">FOSS Academic</title><subtitle>A blog dedicated to two things. First, an exploration of Free and Open Source tools useful to academics. Second, an exploration of the culture, politics, and practices of FOSS.</subtitle><entry><title type="html">Hitting a Nerve and Presenting at IETF</title><link href="http://localhost:4000/2024/07/25/ASM-update.html" rel="alternate" type="text/html" title="Hitting a Nerve and Presenting at IETF" /><published>2024-07-25T00:00:00-04:00</published><updated>2024-07-25T00:00:00-04:00</updated><id>http://localhost:4000/2024/07/25/ASM%20update</id><content type="html" xml:base="http://localhost:4000/2024/07/25/ASM-update.html"><![CDATA[<p>Nothing major this month, but I do want to note two things. First, I wrote a post on the fediverse that must have hit a nerve. It’s about corporations selling off academic work to AI companies. Second, I presented research to a panel at the IETF, talking about the development of ActivityPub by the W3C’s Social Web Working Group. The presentation is based on my <a href="/2024/02/11/Move-Slowy-Preview.html">forthcoming book</a>.</p>

<!-- more -->

<h2 id="hitting-a-nerve">Hitting a Nerve</h2>
<p>A colleague of mine shared <a href="https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai">an article from <em>The Bookseller</em></a> that reports that Taylor and Francis – which is a major publisher of academic articles and books – is selling access to its catalog to Microsoft to train AI. [Note that <em>The Bookseller</em> provides free access to one article per month.]</p>

<p>I got a bit upset and <a href="https://aoir.social/deck/@rwg/112819578544558425">posted this</a> to the fediverse:</p>

<blockquote>
  <p>After 15+ years of hustling to publish #academic journal articles and books, a significant proportion of my work is now being sold off to #Microsoft to train #LLMs. I never consented to this, and of course when I started my career, I never predicted this.
That whoosh you’re hearing is me rushing to sign on to whatever class action lawsuit is in the offing.</p>
</blockquote>

<p>This hit a bit of a nerve on the fediverse, with quite a few folks boosting the post. That tells me I’m not alone in being angry about this.</p>

<p>It turns out that the Microsoft deal isn’t the only one. Taylor and Francis has <a href="https://www.thebookseller.com/news/taylor-francis-set-to-make-58m-from-ai-in-2024-as-it-reveals-second-partnership">announced another</a>. According to <em>The Bookseller</em>, “the business said it plans to reinvest a third of its AI-related profit into ‘accelerated technology, open research and AI product development… based on our unique content.’”</p>

<p>There’s that word. Content. For years I’ve resisted the appelation “content creator.” I find it to be insulting, frankly. I don’t spend my days filling containers. People who operate machines that deposit food into cans are content creators. (No insult meant to people who work in factories!)</p>

<p>But now my “unique content” is going to feed shitty AI bots that no doubt will help write bad journal articles or provide “expertise” via some chat or search applications.</p>

<p>This isn’t what I signed up for. This isn’t what being an open academic who publishes work for others to build on meant to me when I started.</p>

<p>I don’t know if there is any legal recourse. I’ll leave that to lawyers – and if there is a class action suit, sign me up!</p>

<p>But I do know that we academics really ought to withhold our work from massively profitable corporations who view us as content creators. That means no reviewing for T&amp;F, no submissions to T&amp;F journals, and no publishing books with Routledge.</p>

<p>Unfortunately, that means we’ll have to likely boycott SAGE and Elsevier, as well.</p>

<h2 id="ietf-presentation">IETF Presentation</h2>

<p>In happier news, I presented research to the IETF. I’ve never been to an IETF meeting and I think by and large it went well.</p>

<p>I presented a version of what will be the second chapter of my book, which focuses on the intertwined histories of ActivityPub and Mastodon. The presentation was part of a panel of folks studying standardization processes. I was especially impressed by the work of Diane Liao and Michelle Parkouda (as presented by Liao) on ways in which standards bodies could consider differential impacts of standards based on gender.</p>

<p>The panel was recorded – <a href="https://play.conf.meetecho.com/Playout/?session=IETF120-RASPRG-20240724-2000">you can see it here</a>.</p>]]></content><author><name>RWG</name></author><category term="Goal 1" /><category term="Goal 2" /><category term="ActivityPub" /><summary type="html"><![CDATA[Nothing major this month, but I do want to note two things. First, I wrote a post on the fediverse that must have hit a nerve. It’s about corporations selling off academic work to AI companies. Second, I presented research to a panel at the IETF, talking about the development of ActivityPub by the W3C’s Social Web Working Group. The presentation is based on my forthcoming book.]]></summary></entry><entry><title type="html">On Threads’s Blocklist</title><link href="http://localhost:4000/2024/06/28/ThreadsBlocking.html" rel="alternate" type="text/html" title="On Threads’s Blocklist" /><published>2024-06-28T00:00:00-04:00</published><updated>2024-06-28T00:00:00-04:00</updated><id>http://localhost:4000/2024/06/28/ThreadsBlocking</id><content type="html" xml:base="http://localhost:4000/2024/06/28/ThreadsBlocking.html"><![CDATA[<p>Meta’s Threads, which is slowly enabling ActivityPub capabilities, has just posted a <a href="https://www.threads.net/moderated_servers">list of servers it is blocking</a>. The list (as of this writng) has over 660 entries. Since I am finalizing a chapter on Threads for my <a href="/2024/02/11/Move-Slowy-Preview.html">forthcoming book</a>, I will definitely have to consider this list in detail. For now, I wanted to offer a few comments on this development, since it’s a decidedly important moment in the ActivityPub-based fediverse.</p>

<!-- more -->
<h2 id="meta-is-being-transparent-sorta">Meta is being transparent… sorta</h2>
<p>I want to offer the most charitable take I can first: this public blocklist is one of the biggest moments of tranparency that Meta has ever engaged in. Maybe some of my academic colleagues who closely monitor Meta could correct me, but this strikes me as Meta offering quite a bit more insight into content moderation practices than they have in the past.</p>

<p>I think credit is due to the fediverse as a whole on this one, though. If Meta truly wishes to integrate with the fediverse, they have to follow the protocols, and I don’t just mean ActivityPub – they have to follow the <a href="https://www.tandfonline.com/doi/full/10.1080/1369118X.2022.2147400">social protocols of transparent, open moderation</a>. While typically this is in the form of simple, clear codes of conduct and open communication from moderators, posting a blocklist publicly a good step in the direction of transparency.</p>

<h2 id="this-may-be-the-first-time-millions-see-blocking-in-action">This may be the first time millions see blocking in action</h2>
<p>When I first started out on the ActivityPub-based fediverse back in early 2017, most Mastodon instances published their blocklists. Admins can still do this – indeed, Mastodon.social, the largest server, <a href="https://mastodon.social/about">does (scroll down and click on “moderated servers”)</a>.</p>

<p>However, over the past few years, many smaller servers stopped sharing this information publicly. Why? Well, it’s obviously a vector for harassment. For example, if as an admin I publicly say I block bad.instance, their friends could harass me with constant requests about why I did so, arguing that I need to change my mind. I have other stuff to do. Many admins just keep their blocklists to themselves.</p>

<p>So Threads is revealing something many of the millions of new fediverse members have not often seen: the fact that a healthy server <em>must</em> block many others in order to moderate effectively. It’s a fact of fediverse life.</p>

<p>This is a pretty radical change for Meta, to be honest: they now have to declare their relationship to the rest of the web, rather than just pretend that the web must flow through Meta properties.</p>

<h2 id="blocking-nazis-and-nudity">Blocking Nazis and nudity</h2>
<p>Of course, transparency allows for scrutiny. We can take a look at the 660+ blocks and see what sort of values Meta adheres to.</p>

<p>First of all, there are a lot of instances on their list that I completely agree need to be blocked, including overtly white supremacist instances. I won’t specify these, but if you look at the list, you’ll see what I mean just from the URLs. There are also instances geared towards sharing sexualized cartoon images of children (a practice tolerated in Japan, but not by the rest of the fediverse). You can likely guess from those URLs, too.</p>

<p>However, multiple fediverse members have pointed out that Threads is blocking many queer and trans instances. While we’d have to dig in to figure out why, the most likely answer is that Meta has policies against nudity. Indeed, Meta’s own Oversight Board found that <a href="https://gizmodo.com/instagram-meta-age-verification-system-ai-1849753822">Meta is too aggressive in blocking nudity among LBTQ* users</a>, including posts discussing top surgery.</p>

<p>In my interview with Mastodon founder Eugen Rochko, he saw such bans on consenting adult nudity as a very US-centric approach to global content moderation:</p>

<blockquote>
  <p>[On the fediverse], there are thousands of other providers across the entire world, each giving a little bit of social space to the world, that is not controlled entirely from the US. And we know that US politics is not very stable – there’s a lot of turbulence. Trump was in charge, and that was not great for progressive laws.</p>
</blockquote>

<blockquote>
  <p>Some laws were passed, about 5 years ago, that affected a lot of sex workers – the SESTA and FOSTA bills – that basically erased sex work from social media because [so much] social media is based in the US. I think that’s not fair, because in Germany sex work is legal. Sex work <em>is</em> work.</p>
</blockquote>

<p>The Threads blocklist appears to be an extension of this logic: yet another attempt to moderate the globe from California.</p>

<p>Of course, if we think of Threads as another node on the fediverse, well… they can block whomever they wish for whatever reason. But the fact is, Meta is so large that if the rest of the fediverse tries to connect to it, we will defacto have US-based regulations on a highly localized, global network.</p>

<p>Indeed, Rochko’s own Mastodon.social should probably be blocked by Threads, since it allows for pornography. In fact, several folks on the fediverse saw that Mastodon.social was blocked for 30 minutes, though it didn’t seem to affect the connection.</p>

<p>Was that a mistake? Or a warning shot?</p>

<h2 id="you-blocked-what-now">You blocked what now?</h2>
<p>Threads blocking white supremacists and loli comics strikes me as a fine choice. I don’t agree at all with their blocking LGBTQ instances, but if they insist on blocking nudity, I suppose it makes some sense – if only in a bland, shopping mall sort of way.</p>

<p>But some of their blocking choices make no sense to me.</p>

<p>For example, Threads blocks Scholar.social. An instance dedicated to acadmics, Scholar.social is one of the oldest Mastodon instances, and it has been a model of content moderation, safety for marginalized folks, and thoughtful federation practices. I know it very well because it was my home instance for several years. And when I shifted to being an admin of AoIR.social, we asked for permission to copy Scholar’s code of conduct.</p>

<p>The only reason Threads offers for the block is Scholar “Violated our Community Guidelines or Terms of Use.” I have no idea what this could mean. But the good news is, I know for a fact that Scholar would never federate with anything Meta, anyway, so I imagine their being blocked by Meta is a badge of honor!</p>

<p>Other confusing blocking choices that jump out to me are Kolektiva.social and Tabletop.social, two instances I also find to be well-moderated. Again, the justification is violated community guidelines. And, again, I don’t think these servers care, since they are signatories to the <a href="https://fedipact.online/">Anti-Threads Fedipact</a>.</p>

<p>For the rest of us – those of us who haven’t been blocked by Meta – take these arbitrary choices as a warning: if you attempt to federate with Threads, perhaps have a plan for when Threads blocks you and your members.</p>

<h2 id="cross-referencing-ahoy">Cross referencing ahoy!</h2>
<p>Confusing decisions aside, it would be fascinating to cross-reference Threads’s blocklist with shared blocklists, like the Bad Space, Oliphaunt’s list, Sierdy’s, and Garden Fence. I myself drew on these lists when we started AoIR.social. As I mentioned above, this is at least a bit of transparency from a Meta property – we can learn a great deal about what they’re thinking. But we can also learn what the broader fediverse values by comparing Meta’s blocklist to those of other servers as well as shared lists.</p>

<p>As is clear from looking at the list, Meta continues to have a particular vision for global moderation. If you’re looking to emulate Meta-style blocking, you can copy their list. If you reject Meta values, take a long hard look at their list and ask yourself if you should block the same servers.</p>

<p>If this were just any fediverse server, I don’t think this blocklist would draw too much attention. But if it is an indication that this incredibly large, powerful social media corporation intends to reshape the fediverse in its image, and if they insist on blocking several well-moderated servers, it is a disturbing list – not because of the choices they made that are right, but because of the arbitrary choices they make that simply get it wrong.</p>]]></content><author><name>RWG</name></author><category term="Threads" /><category term="Goal 2" /><summary type="html"><![CDATA[Meta’s Threads, which is slowly enabling ActivityPub capabilities, has just posted a list of servers it is blocking. The list (as of this writng) has over 660 entries. Since I am finalizing a chapter on Threads for my forthcoming book, I will definitely have to consider this list in detail. For now, I wanted to offer a few comments on this development, since it’s a decidedly important moment in the ActivityPub-based fediverse.]]></summary></entry><entry><title type="html">Maven Ain’t So Mavenly</title><link href="http://localhost:4000/2024/06/12/Maven.html" rel="alternate" type="text/html" title="Maven Ain’t So Mavenly" /><published>2024-06-12T00:00:00-04:00</published><updated>2024-06-12T00:00:00-04:00</updated><id>http://localhost:4000/2024/06/12/Maven</id><content type="html" xml:base="http://localhost:4000/2024/06/12/Maven.html"><![CDATA[<p>The ever-alert Liaizon Wakest has informed the rest of us on the ActivityPub-based fediverse of a new social media site, <a href="https://www.heymaven.com">Maven</a>, which has <a href="https://app.heymaven.com/discover/1190743">ingested millions of posts from fediverse accounts</a>, including mine.</p>

<p>Multiple people have pointed out how this violates consent on the fediverse. In response, the CTO of Maven, Jimmy Secretran, <a href="https://mastodon.social/@jsecretan/112604200409755575">has explained their reasoning</a>:</p>

<blockquote>
  <p>We are trying to connect up to the Fediverse, to allow interaction with other ActivityPub servers.  This definitely seems to me to be within the spirit of what ActivityPub enables, but of course, I don’t want to have Maven connect to anybody who doesn’t want it.</p>
</blockquote>

<p>[Note that I normally do not quote fediverse posts without permission, but in this case, I am making an exception, for reasons that I think will be obvious.]</p>

<p>I replied in the thread, arguing that, no, they are not really abiding by the spirit of ActivityPub:</p>

<blockquote>
  <p>This isn’t how this works. No one starts a fediverse (AP) server by ingesting a bunch of posts from others without their consent. They start servers and start federating with the rest of the network.
Please stop ingesting posts from AoIR.social (I’m the admin, btw).</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>The custom is to start a server with a code of conduct, including clear moderation rules, so that the rest of us can make informed choices about federating. What you’ve done with Maven is a pretty massive violation of norms, and likely it will result in your being defederated from many other instances. It’s a poor way to start an ActivityPub implementation.</p>
</blockquote>

<p>To be fair to Secretran and Maven, they have since stopped scraping my posts and, I presume, those of others who have asked them to stop. Still, I eagerly await Maven’s full ActivityPub implementation so that we can block them effectively.</p>

<p>This incident got me to thinking about norms and customs on the fediverse and how important they are.</p>

<!-- more -->

<h2 id="states-of-exception-versus-the-big-empty">States of Exception versus The Big Empty</h2>
<p>While the Maven incident was happening, the Mastodon instance I’m helping to run, AoIR.social, is gearing up for a wave of new members. The reason is that registration for the <a href="https://aoir.org/aoir2024/">2024 Association of Internet Researchers conference</a> is now open. Every paid AoIR member has access to our Mastodon server, and with every conference we get new folks who join the Association. So, we’re going to get new folks on the server. Our idea is that, since Twitter is now utterly garbage, our members need a new microblogging home to share their work.</p>

<p>There’s an important contrast to be drawn between the Maven incident and the new wave of AoIR.social members. As a start-up, Maven appears to have adopted a classic Silicon Valley logic of the “<a href="https://firstmonday.org/ojs/index.php/fm/article/view/2799">state of exception</a>,” where one must quickly grow at all costs. One way to supercharge your growth is to simply steal content from other sources. It worked for YouTube back in the 2000s and for AI companies more recently. In the case of Maven, given its AI-based approach, it’s pretty clear the goal was to scrape a bunch of social content from an easy source (Mastodon.social) to train their models upon to produce their “interest-based” social media service.</p>

<p>In contrast, joining a Mastodon server for the first time is a bit of lonely experience. I can imagine that many new AoIR.social members may be looking at their big, empty feeds and wondering what’s next. Whom should I follow? How do I find my colleagues? Will anyone follow me? Even with things like trending topics and recommended accounts to follow, Mastodon and the fediverse is not going to do much to shape what a new user does. In the interviews I’ve conducted <a href="/2024/02/11/Move-Slowy-Preview.html">for my book</a>, I heard from many folks who said joining the fediverse was much colder than joining a corporate social media site.</p>

<p>So, perhaps the steal-and-grow approach is best? Perhaps Maven is justified to ingest a bunch of content to present to would-be new users? Doing so could jumpstart the hard process of making yet another social media site.</p>

<p>But I don’t think so.</p>

<h2 id="the-irony-of-maven">The Irony of Maven</h2>

<p>Let’s consider the supreme irony of Maven’s ingestion of posts without consent. As <a href="https://www.heymaven.com/about">they explain on their “About” page</a>:</p>

<blockquote>
  <p>It’s not right that every alternative to the popularity contest on one site is just a popularity contest on another.  We deserve a place to go where you can connect to people simply because you share interests, where you can speak without 100% confidence because you aren’t “performing” in front of a bunch of judges, where you can simply hold a conversation because you’re curious about the subject matter, where everyone isn’t there to build a brand and become an influencer.</p>
</blockquote>

<p>That is, they’re suggesting that social media is mired in empty performances in order to gain clout, but thanks to their innovative AI, we can have social media without such clout-chasing. Instead of what’s <em>popular</em>, we’ll get what’s <em>good</em>:</p>

<blockquote>
  <p>Part of the idea behind Maven is that there is an enormous flux of ideas and interests out there on the internet that we never get to see or interact with because everything we experience is always based on popularity.  That changes with Maven.</p>
</blockquote>

<p>This would mitigate</p>

<blockquote>
  <p>the fact that only a tiny elite 0.1% of the population (with say 10K followers or more) can get any substantial amount of engagement while everyone else are just supporting cast members.</p>
</blockquote>

<p>That all sounds great: it’s addressing a key concern people have about corporate social media, that it’s just a platform for a handful of famous influencers.</p>

<p>But here’s the irony: in order to get their service off the ground, they ingested millions of posts from the fediverse to make their service seem… <em>more popular than it actually is.</em> They’re using fediverse content to gain attention. Hell, they’ll probably get some media coverage out this, at least.</p>

<p>In contrast, the new member of AoIR.social, staring down a blank feed, might think that the fediverse isn’t for them. There’s nothing to jumpstart the process of building out a presence on the fedi.</p>

<p>But the new fedi member posts once, and gets a reply. Then again, and some followers. Over time, with a bit of work, they’re entering into conversations.</p>

<p>And for those who want to start a fediverse instance, that will <em>really</em> be empty. But if you work at it – making a code of conduct, inviting people to join, doing the work of moderating – then your server will integrate into the fediverse.</p>

<p>Those are the norms of the fediverse: do the work. Explore. Reach out. Moderate. Learn. Grow slowly.</p>

<p>And don’t steal the work of others.</p>

<p>By effectively stealing posts from the fediverse in order to look more popular than it is, Maven has utterly blown their chance to be the alternative they claim they want to be. And even if they have stopped scraping and do a proper ActivityPub implementation, they’re going to be #fediblocked pretty severely from here on out.</p>]]></content><author><name>RWG</name></author><category term="Maven" /><category term="Ethics" /><summary type="html"><![CDATA[The ever-alert Liaizon Wakest has informed the rest of us on the ActivityPub-based fediverse of a new social media site, Maven, which has ingested millions of posts from fediverse accounts, including mine. Multiple people have pointed out how this violates consent on the fediverse. In response, the CTO of Maven, Jimmy Secretran, has explained their reasoning: We are trying to connect up to the Fediverse, to allow interaction with other ActivityPub servers. This definitely seems to me to be within the spirit of what ActivityPub enables, but of course, I don’t want to have Maven connect to anybody who doesn’t want it. [Note that I normally do not quote fediverse posts without permission, but in this case, I am making an exception, for reasons that I think will be obvious.] I replied in the thread, arguing that, no, they are not really abiding by the spirit of ActivityPub: This isn’t how this works. No one starts a fediverse (AP) server by ingesting a bunch of posts from others without their consent. They start servers and start federating with the rest of the network. Please stop ingesting posts from AoIR.social (I’m the admin, btw). and The custom is to start a server with a code of conduct, including clear moderation rules, so that the rest of us can make informed choices about federating. What you’ve done with Maven is a pretty massive violation of norms, and likely it will result in your being defederated from many other instances. It’s a poor way to start an ActivityPub implementation. To be fair to Secretran and Maven, they have since stopped scraping my posts and, I presume, those of others who have asked them to stop. Still, I eagerly await Maven’s full ActivityPub implementation so that we can block them effectively. This incident got me to thinking about norms and customs on the fediverse and how important they are.]]></summary></entry><entry><title type="html">Thinking of Switching to the Linux, Fellow Academics?</title><link href="http://localhost:4000/2024/05/24/fald.html" rel="alternate" type="text/html" title="Thinking of Switching to the Linux, Fellow Academics?" /><published>2024-05-24T00:00:00-04:00</published><updated>2024-05-24T00:00:00-04:00</updated><id>http://localhost:4000/2024/05/24/fald</id><content type="html" xml:base="http://localhost:4000/2024/05/24/fald.html"><![CDATA[<p>Ok, so Microsoft did a thing, a potentially privacy-violating thing, <a href="https://mashable.com/article/microsoft-recall-ai-feature-uk-investigation">called Recall</a>. Microsoft has been doing <a href="https://www.theverge.com/2024/4/24/24138949/microsoft-windows-11-start-menu-ads-recommendations-setting-disable">lots of other things</a>. Apparently these things have led to people <a href="https://www.pcmag.com/news/10-reasons-not-to-upgrade-to-windows-11">not wanting to use Windows 11, the latest Windows</a> – or even stop using Windows altogether.</p>

<p>It seems people are upset – so much so, they are talking about switching to Linux. For those of you who are academics (or in related fields), I can tell you: the switch is entirely feasible.</p>

<p>I’ve been using Linux since about 2008 or so to do my academic work. After writing <a href="https://search.worldcat.org/search?q=robert+w+gehl&amp;author=Gehl+Robert+W&amp;itemType=book&amp;itemSubType=book-printbook%2Cbook-digital%2Cbook-thsis">three books</a> with a <a href="2024/02/11/Move-Slowy-Preview.html">fourth on the way</a>, as well as nearly <a href="https://www.robertwgehl.org/publications.php?styleSheetSelection=mobile">30 peer-reviewed articles</a>, I’d say it’s clear the work can be done with Linux.</p>

<p>I should note I’m in the humanities, not computer science, so I’m using Linux mainly to write, research, and collaborate with other authors doing social science work. And it’s also important to note that I have worked at “Microsoft Campuses” for my whole career, meaning most of my colleagues use Windows and the IT folks support mostly Windows. In fact, when IT finds out I use Linux, they tend to say “We don’t officially support that” (and then they help as best they can). So I am definitely proof an academic can survive using Linux.</p>

<p>Here, I want to talk a bit about my experience using Linux and Free and Open Source Software (FOSS) as an academic for the past two decades and toss in some tips for those thinking of switching. I hope it’s helpful, and if you have questions, let me know!</p>

<p>(Also, before I begin: use whatever computer OS you like! I’m not judging! I just want to help academics who might want to switch.)
<!-- more --></p>
<h2 id="use-an-older-computer">Use an older computer</h2>
<p>One thing I’ve enjoyed over the years of using Linux is that it works great on older computers. These days, when I buy a new computer, I expect to get about 5-10 years of performance out of it. This keeps computers out of landfills, and it helps me with things like writing routines.</p>

<p>So, if you’re considering switching from Windows to Linux, get an old laptop (buy a used one, or grab one you have in a closet) and install Linux on that one. That way, you’re not stressed out about losing data. Even if you make a mistake in installing Linux, you’re not out of luck. You can start out small, doing some work tasks on the Linux machine with the safety net of the Windows machine as support. Over time, you can migrate data and accounts to the new machine until you’ve developed a new workflow.</p>

<p>Another approach that’s similar is “dual booting” – that is, using the same computer but with two operating systems (e.g., Windows and Linux). You pick which one you want to use when you start the computer. Many Linux distributions will guide you through installing Linux alongside Windows. You keep your old data this way, but you have a new operating system to experiment with.</p>

<h2 id="or-treat-yourself">Or treat yourself!</h2>
<p>Or, if you’ve got the funds, buy a computer with Linux pre-installed. There are companies that specialize in this, like <a href="https://system76.com/">System76</a> in the US. And big computer companies, such as Dell and Lenovo, offer Linux options. You can get anything from budget machines to really high-end computers with Linux pre-installed.</p>

<h2 id="pick-a-distribution-of-linux">Pick a distribution of Linux</h2>
<p>Probably the most confusing part of this is the fact that there isn’t just one Linux – there are many “distributions” (“distos” for short). The good news is most of them are great for people switching from Windows, because that’s a really common path people take.</p>

<p>If you buy a new computer from a company like Dell or System76, they will do the work of selecting a distribution for you. System76 uses one called <a href="https://pop.system76.com/">Pop OS</a>, and Dell offers <a href="https://ubuntu.com/desktop">Ubuntu</a>. Those are great choices!</p>

<p>And there are others. I myself am using <a href="https://fedoraproject.org/">Fedora</a>, specifically one with a user interface called <a href="https://kde.org/">KDE</a>.</p>

<p>And you can switch later! If you use one distribution, and you want to try others, you can. It’s called “distro hopping” and I do it for fun.</p>

<p>But! I don’t want you to be overwhelmed. Basically, I’d suggest picking one (Ubuntu is always a safe choice) and using it as you learn. Don’t stress too much about this.</p>

<h2 id="find-a-buddy-tune-out-the-zealots">Find a buddy, tune out the zealots</h2>
<p>Linux users aren’t as common as Windows users, but we’re out there. Chances are, you’ve got a colleague who also uses Linux. Having a buddy can be useful if you run into trouble. That buddy might even be in your university’s IT department, which might be useful if you need support.</p>

<p>A flipside of this: tune out zealots. Linux (and FOSS in general) can have folks getting quite pushy and judgy. If you use Ubuntu, someone might scoff and say “I use Arch.” Ignore these people.</p>

<h2 id="use-whatever-apps-you-need">Use whatever apps you need</h2>
<p>As I mentioned above, I’ve worked at Microsoft-dominated universities for my career. This means many of my colleagues use Teams, and Outlook is our provided email system. The good news is Teams works on Linux, and Outlook emails and calendaring can either be done online or with a FOSS client like <a href="https://www.thunderbird.net/en-US/">Thunderbird</a>. Word documents and Powerpoint slides can be opened and edited with either an online editor or with <a href="https://www.libreoffice.org/">LibreOffice</a>.</p>

<p>There’s a Zoom client for Linux, too.</p>

<p>The bottom line is: even if you’re using Linux, you don’t have to entirely abandon applications if you need to use them.</p>

<p>I myself use these things, although I also consider myself a sort of experimenter, and I try to find FOSS alternatives, such as Nextcloud, to do the same tasks. No matter what task you’re trying to take care of, there’s probably a FOSS alternative that can help. But you <em>don’t</em> have to use the alternative in many cases. You can carry on using the apps you’re used to.</p>

<h2 id="live-the-dream">Live the dream</h2>
<p>With Linux installed, you can live the dream!</p>

<p>I call it the <strong>FOSS Academic Lifestyle Dream.</strong> The benefits of using Linux and FOSS are many! Whenever I hear about Microsoft or Apple doing something weird, I can tune it out, because I now have so much more autonomy over my computers. Whenever I hear about this or that application selling off personal data or using it train AI without consent, I can tune that out, too – these things don’t happen with FOSS. I enjoy a lot more privacy and flexibility as I use my computers, and I’ve learned a lot, too.</p>

<h2 id="deeper-dives">Deeper Dives</h2>
<p>If you liked this post, I have previous posts talking about my workflow and the software I use:</p>
<ul>
  <li>A <a href="/2021/05/14/FALDupdate.html">discussion of Nextcloud</a>,</li>
  <li>a <a href="/2022/02/03/Zettlr.html">note about Zettlr for note-taking</a>,</li>
  <li>two posts on Zotero, including <a href="/2020/12/01/Zotero-Tips-and-Tricks.html">tips and tricks</a> and a <a href="2022/10/05/deepdivezotero.html">deeper dive</a>, and</li>
  <li>an <a href="/2020/12/15/FOSS-Journey.html">overview of how I started using FOSS</a> (note how I was inspired by a bad Windows experience).</li>
</ul>]]></content><author><name>RWG</name></author><category term="Goal 1" /><category term="FALD" /><summary type="html"><![CDATA[Ok, so Microsoft did a thing, a potentially privacy-violating thing, called Recall. Microsoft has been doing lots of other things. Apparently these things have led to people not wanting to use Windows 11, the latest Windows – or even stop using Windows altogether. It seems people are upset – so much so, they are talking about switching to Linux. For those of you who are academics (or in related fields), I can tell you: the switch is entirely feasible. I’ve been using Linux since about 2008 or so to do my academic work. After writing three books with a fourth on the way, as well as nearly 30 peer-reviewed articles, I’d say it’s clear the work can be done with Linux. I should note I’m in the humanities, not computer science, so I’m using Linux mainly to write, research, and collaborate with other authors doing social science work. And it’s also important to note that I have worked at “Microsoft Campuses” for my whole career, meaning most of my colleagues use Windows and the IT folks support mostly Windows. In fact, when IT finds out I use Linux, they tend to say “We don’t officially support that” (and then they help as best they can). So I am definitely proof an academic can survive using Linux. Here, I want to talk a bit about my experience using Linux and Free and Open Source Software (FOSS) as an academic for the past two decades and toss in some tips for those thinking of switching. I hope it’s helpful, and if you have questions, let me know! (Also, before I begin: use whatever computer OS you like! I’m not judging! I just want to help academics who might want to switch.)]]></summary></entry><entry><title type="html">Decentralization or Noncentralization, Bluesky or the Fediverse?</title><link href="http://localhost:4000/2024/04/30/DecentralizedNoncentralized.html" rel="alternate" type="text/html" title="Decentralization or Noncentralization, Bluesky or the Fediverse?" /><published>2024-04-30T00:00:00-04:00</published><updated>2024-04-30T00:00:00-04:00</updated><id>http://localhost:4000/2024/04/30/DecentralizedNoncentralized</id><content type="html" xml:base="http://localhost:4000/2024/04/30/DecentralizedNoncentralized.html"><![CDATA[<p>I’m nearing wrapping up drafting <em>Move Slowly and Build Bridges</em>, <a href="/2024/02/11/Move-Slowy-Preview.html">my book about the fediverse</a>. I do several things in the book: provide some history, show the struggles of instance admins, talk about the politics of blocklists.</p>

<p>I also develop a concept in the book, “noncentralization.” In this blog post, I want to talk about what this means and contrast the term with the more popular one, “decentralization.” To do that, I want to contrast the fediverse with another system, Bluesky.</p>

<!-- more -->

<h2 id="decentralization-versus-noncentralization">Decentralization versus Noncentralization</h2>
<p>As I argue, the very term “decentralized” implies that there was once a center, something that has been broken up, <em>de</em>-centered. This process could work in reverse – something decentralized could be <em>re</em>-centralized at a later point. It might be hard to do, but it’s implied by the idea.</p>

<p>In contrast, <em>non</em>-centralized means just that: designed to avoid a center emerging. It’s a design goal from the outset. (Whether it is successful is another matter.)</p>

<p>I get this term from political philosophy, specifically federalism theory. As Glen Moots argues in <em><a href="https://www.routledge.com/The-Ashgate-Research-Companion-to-Federalism/Ward/p/book/9780367603021">The Ashgate Research Companion to Federalism</a></em>, in federalist political structures,</p>

<blockquote>
  <p>Power is diffused. Centralizing or concentrating power in a federalist system risks breaking the structure and spirit of the constitution. It negates the whole telos of federalism. Non-centralization applies to the United States, Canada, and Switzerland, for example. In each case, there is no central government that controls all the lines of political communication and decision-making. States, cantons, or provinces are not creatures of the federal government (Elazar 1984b 14).</p>
</blockquote>

<p>We might argue whether or not the US or Canada is truly non-centralized. But the point Moots is making is that the ideal political structure envisioned by federalism doesn’t involve breaking up an existing center. As Moots writes, “decentralization implies the existence of a central authority that can decentralize or recentralize as it pleases.” Noncentralization by contrast involves designing political structures so that there are many, smaller locations where power is exercised. This allows for a high degree of local autonomy coupled with a larger-scale agreement about shared ethical values. (In the case of the fediverse, I call such a structure “the covenantal fediverse,” but that’s a topic for another post.)</p>

<p>Now, in my work on alternative social media, I’m not talking about states. I’m talking about social media. But given that social media must be governed (e.g., moderated, have policies), and given that we’re now living in a time of federated social media, I think it’s a good idea to look to federalist political theory to think about how things might work.</p>

<p>The two main projects seeking to federate social media today are the ActivityPub-based fediverse and Bluesky. Drawing on the noncentalized/decentralized distinction, I would argue that Bluesky is decentralized – and as such will never truly be federated. The ActivityPub-based fediverse is closer to the noncentalized ideal. But it is vulnerable to centralization, too.</p>

<h2 id="bluesky-and-decentralization">Bluesky and decentralization</h2>
<p>Bluesky has the ambition to be a decentralized Twitter. To that end, Bluesky has started out centralized and is slowly spinning out parts of itself from the center to the edges.</p>

<p>A simple example is support for GIFs. Bluesky has <a href="https://bsky.app/profile/bsky.app/post/3kqy4mjw5eo2t">proudly declared they now let people post GIFs</a>… <em>powered by Tenor</em>. What’s Tenor? It’s a <a href="https://en.wikipedia.org/wiki/Tenor_(website)">Google-owned GIF repository and search engine</a> with <a href="https://tenor.com/gifapi/documentation">an API</a>. So people can “post” GIFs to Bluesky insofar as that means they pull in a GIF from another, centralized database of them and have it embedded in the stream.</p>

<p>In keeping with decentralization, Bluesky says that “custom GIFs” will be implemented later. I take that to mean they will allow uploading a .gif file from a local machine. You can’t do that now (I tried, and it just is a still image). So for now GIF posting is decentralized in that it involves two major services (Bluesky and Google’s Tenor), and in the future these centers might get broken up further.</p>

<p>It’s hard not to see this as symptomatic of Bluesky’s approach as a whole. It started out as one central service, Bsky.app, with the promise that people could run their own Personal Data Servers (PDSes) in the future (a future that is, to their credit, slowly happening).</p>

<p>But Bluesky distinguishes between “speech” and “reach,” meaning that while people can run their own PDSes and speak, their posts won’t appear across the network unless they are indexed or relayed – and Bluesky runs the only indexer. They promise that in the future that non-Bluesky employees can run indexers/relays, too, but that’s a costly proposition – much more so than running a PDS.</p>

<p>In the meantime, Bsky.app, the central Bluesky server, includes a connection to Twilio Segment, which is <a href="https://segment.com/">“The leading customer data platform, powered by CustomerAI”</a>.</p>

<figure>
  <img src="/assets/images/bskySegment.png" alt="A screenshot of my Noscript Firefox extension on bsky.app, showing Segment.com as a source for a script" />
  <figcaption>A screenshot of my Noscript Firefox extension on bsky.app, showing Segment.com as a source for a script</figcaption>
</figure>

<p>Segment’s presence on the site raises the specter of behavioral advertising – a possibility the company has never denied. Consider this Q and A between <a href="https://www.wired.com/story/bluesky-ceo-jay-graber-wont-enshittify-ads/">Bluesky CEO Jay Graber and <em>WIRED</em></a>:</p>

<blockquote>
  <p><strong>Are you thinking about advertisements at all?</strong></p>
</blockquote>

<blockquote>
  <p>There will always be free options, and we can’t enshittify the network with ads. This is where federation comes in. The fact that anyone can self-host and anyone can build on the software means that we’ll never be able to degrade the user experience in a way where people want to leave.</p>
</blockquote>

<p>That’s not a “no.” It’s hard to understand exactly what it is.</p>

<p>It might be, “we won’t do ads.” But then what’s the Segment integration for? Segment’s presence might just mean Bluesky will sell data to marketers but not allow ads. (After all, brands are certainly welcome on Bluesky and may want insights into their audiences.)</p>

<p>Or, it might be, “if we do ads <em>wrong</em> people will leave, so we better do them right.” That seems likely given the Segment integration.</p>

<p>Or maybe Graber’s answer means, “if we do allow ads, people can just move to another PDS and appview.” That is, we can run ads on our PDS/appview, but others may decide not to.</p>

<p>In any case, if the costs of running Bluesky infrastructure can be partially subsidized by ads or other behavioral marketing techniques (without “enshittifying” everything), then most likely there will be one central mechanism by which anyone wishing to subsidize their part of the network (e.g., a distinct PDS) can do so. If Bluesky runs this monetization system, then de facto third-party PDSes that use it will be closely tied to the center.</p>

<p>Decentralization also holds when we consider the underlying protocol Bluesky is making, ATProto, That’s being done by them alone. They promise it will be transferred to a standards body (e.g., the IETF) in the future so that it is more open. It could be that they do this, but it would replicate the pattern: they start with a center and then decentralize. The protocol will be made in-house, and even after being moved to something like the IETF, it can be dominated by Bluesky the company.</p>

<p>Overall, Bluesky is decentralized in the sense I’m using it here: there’s a center that’s being broken up. This might turn out for the best. <a href="https://steveklabnik.com/writing/how-does-bluesky-work">Steve Klabnik’s explainer</a> has a similar (if more sanguine) take:</p>

<blockquote>
  <p>…the BlueSky team has demonstrated, in my personal opinion, enough understanding and uncomfortableness with being in control here, and it’s designed in such a way that if other, better systems develop, you can move to them. They’ve also indicated that moving governance of did:plc to some sort of consensus model in the future is possible. There are options. Also, others could run a did:plc service and use that instead if they prefer, too.
I personally see this as an example of pragmatically shipping something, others see it as a nefarious plot. You’ll have to decide for yourself.</p>
</blockquote>

<p>So it’s a pragmatic choice to do everything centrally, ship it now, and devolve it to the edges over time.</p>

<p>But it seems to me that, no matter what, much will continue to flow to the center, either on purpose or through inertia. And edge-parts that misbehave (by Bluesky’s standards) might be recallable to the center. For example, if allowing just anyone to run an indexer causes problems down the road, Bluesky’s control could be re-asserted later.</p>

<h2 id="noncentralization-and-the-fediverse">Noncentralization and the fediverse</h2>
<p>I would argue that the ActivityPub-based fediverse is much closer to the noncentalized ideal I would prefer. By “fediverse” I mean the ActivityPub-based fediverse.</p>

<p>How?</p>

<p>Well, for one thing, there’s a whole host of different projects. From PeerTube to the Wordpress plugin, it’s a network of heterogenous social media services. ActivityPub allows for a range of technologies. Moreover, ActivityPub is already an open standard – it was that way from day 1. It supports a bewildering array of social media projects.</p>

<p>Now, you might argue “Mastodon is dominant.” Yes, Mastodon is dominant, but it’s not the only game in town. And even <em>if</em> the fediverse was mostly Mastodon, what does that mean? There are forks of Mastodon (Glitch, Hometown) and heavily modified Mastodon installations (awoo.space). Mastodon isn’t one thing – it’s many things.</p>

<p>You might say that there are points of centralization at the instance level: that an instance is a center. But they are not central to the network. There are tens of thousands of them, ranging from instances-of-one to large instances. And there’s no guarantee they will connect – they might have ethical disagreements. The smallest instance can block the biggest one. Indeed, that’s why instance blocking exists.</p>

<p>Now, you might say, “What about Threads?” And <em>that</em> is a concern of mine. The existing fediverse is so small that the appearance of a corporate-controlled network with 100 million+ accounts means that de facto there is now a center. That is indeed a problem.</p>

<p>There are solutions, though. One is #<a href="https://fedipact.online/">fedipact-like agreements</a> to block Meta. I endorse this. In my view, the whole point of federated, alternative social media is to be a better alternative to corporate social media, not a means to connect to it.</p>

<p>Another approach to Threads is to hope other big companies get involved in the fediverse to balance things out with Meta. With Meta here, let’s get Apple and Google and Microsoft to adopt ActivityPub. To cite Evan Prodromou, that’s a <a href="https://evanp.me/2023/12/26/big-fedi-small-fedi/">“Big Fedi” solution</a>.</p>

<p>I don’t like this solution, because I would rather see the fediverse be comprised of tens or even hundreds of thousands of small, self-governing communities. That would be closest to a noncentalized ideal: it remains a system where there was no center in the first place, where many small communities band together into a big network through mutual, shared ethical values, and any attempt to centralize will be resisted.</p>

<h2 id="bluesky-wont-be-federated">Bluesky won’t be federated</h2>
<p>Reading the documents and discourses, it’s clear to me Bluesky won’t be federated – it will be decentralized. It won’t be federated because it won’t be noncentralized. It is not being designed that way, and I do not believe it will ever be that way. Some of its centers are going to hold, which negates the idea of distributing power and control. And those things that are spun out to the edges could be recallable in particular ways.</p>

<p>The fediverse is closer to the ideal of noncentralization, but of course Threads indicates that such a thing is fragile. Designing a noncentralized system is hard work, and it can fail.</p>]]></content><author><name>RWG</name></author><category term="Bluesky" /><category term="Fediverse" /><category term="Goal 2" /><summary type="html"><![CDATA[I’m nearing wrapping up drafting Move Slowly and Build Bridges, my book about the fediverse. I do several things in the book: provide some history, show the struggles of instance admins, talk about the politics of blocklists. I also develop a concept in the book, “noncentralization.” In this blog post, I want to talk about what this means and contrast the term with the more popular one, “decentralization.” To do that, I want to contrast the fediverse with another system, Bluesky.]]></summary></entry><entry><title type="html">Reading the Online Harms Act with my Fediverse Admin Hat On</title><link href="http://localhost:4000/2024/04/10/Online-Harms-Act.html" rel="alternate" type="text/html" title="Reading the Online Harms Act with my Fediverse Admin Hat On" /><published>2024-04-10T00:00:00-04:00</published><updated>2024-04-10T00:00:00-04:00</updated><id>http://localhost:4000/2024/04/10/Online%20Harms%20Act</id><content type="html" xml:base="http://localhost:4000/2024/04/10/Online-Harms-Act.html"><![CDATA[<p>The <a href="https://www.canada.ca/en/canadian-heritage/services/online-harms.html">Online Harms Act</a> is currently the talk of Canada. As the government’s website describes it,</p>
<blockquote>
  <p>The internet is an exceptional tool for people of all ages to learn, play and connect with family, friends and those with similar interests. However, just like the outside world, the digital world can pose significant dangers. Social media can be used to sexually exploit children, promote self-harm to children, incite violence, put people’s safety at risk and foment hate. Online harms have real-world impacts with tragic, even fatal, consequences.</p>
</blockquote>

<blockquote>
  <p>The Government of Canada has introduced legislation to hold social media platforms accountable for addressing harmful content on their platforms and for creating a safer online space that protects all people in Canada, especially kids.</p>
</blockquote>

<p>As something that addresses speech on the internet – and includes some strong penalties for spreading hate speech – the Act has led to massive debates. I won’t summarize them here, but you can read thoughtful critiques from <a href="https://www.michaelgeist.ca/tag/online-harms-act/">Professor Michael Geist</a> and you can listen to a discussion on <a href="https://www.canadaland.com/podcast/85-the-hate-u-post/">Canadaland’s Backbench</a> to get a sense of things.</p>

<p>My interest here is not to delve into the controversies, but instead read the Act while wearing my Mastodon admin hat. I am one of the two admins of AoIR.social, a <a href="https://en.wikipedia.org/wiki/Mastodon_(social_network)">Mastodon</a> instance for members of the <a href="https://aoir.org/">Association of Internet Researchers</a>. AoIR.social gives our members access to <a href="https://en.wikipedia.org/wiki/Fediverse">the fediverse</a> – the global network of thousands of social media servers, with millions of users engaged in social media activities.</p>

<p>An initial question: does the Act apply to the tens of thousands of fediverse instances with their millions of users? Does it thus apply to AoIR.social? Considering that the Act defines “social media service” as “a website or application that is accessible in Canada, the primary purpose of which is to facilitate interprovincial or international online communication among users of the website or application by enabling them to access and share content” (page 5), then the answer is yes. AoIR.social operates internationally, including in Canada. So do many of the other tens of thousands of fediverse servers.</p>

<p>Since I’m one of the two admins of AoIR.social, am I affected? I believe so: I would be an “operator,” “a person that, through any means, operates a regulated service.” It could also be that the AoIR organization itself would be the operator, since in this Act “person” also applies to corporations.</p>

<p>The Act would create a Digital Safety Commission, an agency that would govern social media services. The Act notes this Commission should consider things like the size of the service and the operator’s financial and technical abilities. This is good – many fediverse instances are small and are run by individuals or a handful of folks. They also often run on very small budgets as not-for-profit organizations. Regardless, however, I believe small fediverse servers would be affected (unless they find a way to not be available to Canadians).</p>

<p>So: will the Act radically affect fediverse instances, such as AoIR.social? The Act specifically says our service should not harm freedom of expression, and reading it leads me to believe that AoIR.social won’t be affected in that manner. While we welcome debate and discussion, we’re not in the business of platforming the sorts of harmful content the Act discusses.</p>

<p>But, the implementation of the Act could prove to be very burdensome to the administration of fediverse instances, such as AoIR.social. Much is going to hinge on how the Act understands the fediverse.</p>

<!-- more -->

<h2 id="duties-of-operators-of-regulated-services">Duties of Operators of Regulated Services</h2>
<p>The Act is meant to regulate social media, and it lays out various duties social media service operators must follow.</p>

<p>The first is the duty to act responsibly: “The operator of a regulated service must implement measures that are adequate to mitigate the risk that users of the service will be exposed to harmful content on the service.”  Another is a duty to protect children, and related to both, there’s also the duty to make harmful content inaccessible to Canadians.</p>

<p>As a fediverse admin, I have two key questions about these duties. First, what is “harmful content”? This is defined on page 4 in the definitions section:</p>
<ul>
  <li>“(a) intimate content communicated without consent;</li>
  <li>(b) content that sexually victimizes a child or revictimizes a survivor;</li>
  <li>(c) content that induces a child to harm themselves;</li>
  <li>(d) content used to bully a child;</li>
  <li>(e) content that foments hatred;</li>
  <li>(f) content that incites violence; and</li>
  <li>(g) content that incites violent extremism or terrorism.”</li>
</ul>

<p>So, things like revenge “porn”, child sexual abuse material, hate speech, and terrorist speech are prohibited. There are definitions offered for each of these, so I won’t define them here. Frankly, I have no problem blocking these things.</p>

<p>The other question I have is about the responsibility to “mitigate the risk that users of the service will be exposed to harmful content….” It seems to me that this is not just about removing content that’s posted. It’s about reducing the likelihood AoIR.social members will see such harmful content.</p>

<p><em>This</em> is where I have a lot of concerns. Some things seem easy to do, but there are other things that I’m not sure about.</p>

<p>Put simply, the TL;DR summary of the issue is: does the Act consider something like a fediverse server an independent social media site? Or would the Act see the fediverse as a singular network, with traffic being load-balanced across multiple servers?</p>

<p>Let’s get into the details.</p>

<h2 id="whats-easily-done">What’s Easily Done</h2>
<p>The Act calls for social media to publish “Guidelines.” These “must be accessible and easy to use and must include (a) a standard of conduct that applies to users with respect to harmful content; and (b) a description of the measures that the operator implements with respect to harmful content on the service.”</p>

<p>This is pretty easy to comply with, because like many other Mastodon instances, <a href="https://aoir.social/about">AoIR.social has a code of conduct</a>. I believe our code is “accessible and easy to use,” and it explicitly establishes a standard of conduct, which includes prohibitions on the sorts of content the Act describes.</p>

<p>The Act also calls for social media to provide users with “tools to block users” – done and done, because this was baked into Mastodon from its early days.</p>

<p>And the duty to protect children is quite easy for AoIR.social since our membership is limited to dues-paying members of the Association of Internet Researchers, and I don’t think that population includes anyone under 20, let alone minors.</p>

<h2 id="whats-less-clear">What’s Less Clear</h2>
<h3 id="tools-and-processes-to-flag-harmful-content">Tools and processes to flag harmful content</h3>
<p>One feature called for is to “enable a user to easily flag to the operator content that is accessible on the service as being a particular type of harmful content.” That’s baked into Mastodon, so easy enough.</p>

<p>But some of it is not as easy to do, such as “notify a user who flagged content as being a particular type of harmful content of the operator’s receipt of the flag as well as of any measures taken by the operator with respect to the content or of the fact that no measures were taken.” Mastodon does not automatically do this, so it would have to be something the admin/mod does on their own. On AoIR.social, if a user flags something and we block it, we don’t really bother to go back to the original flagger and say we did so. We just do it and move on. I suppose we can do the extra step, but it will add work.</p>

<p>But some of it seems misguided to me: “notify a user who communicated content that was flagged as being a particular type of harmful content of the fact that the content was flagged as well as of any measures taken by the operator with respect to the content or of the fact that no measures were taken.” Mastodon <em>definitely</em> does not do this. I believe this is by design, because notifying people that they have had their content removed can invite more harassment. The approach on Mastodon is to block and not really let the blocked person know. I believe this is for safety purposes –- if we block someone’s content and tell them, they might harass us.</p>

<p>That’s not likely to happen if it’s an AoIR.social member. But the big issue is: <strong>does the Act require me to do these actions with people who are not on my server?</strong> If this is all meant to take place <em>within</em> my server, it’s probably not a problem. Most such in-server moderation is going to involve a discussion with the “user who communicated content that was flagged.” In the case of a user who posts something abhorrent, they’re going to be booted. In the case of someone who posts something borderline, we’re going to chat about it. It’s not going to be a formal process, but more personal.</p>

<p>However, if this is meant to govern my interactions with a user on a server <em>outside</em> my instance, I don’t think this is wise, and it could backfire. If, for example, some trolls are harassing people on AoIR.social, I might communicate with them to tell them to stop, but most likely I’m just going to block them (and hence remove their posts) and not explain myself. Explaining myself would probably lead to more harassment, both for the original victim and likely for me or other AoIR.social moderators.</p>

<p>How the Act handles this is going to matter. If the Act looks at the fediverse and sees a bunch of independent servers which can connect to one another and can sever those connections but have no power over each other, that’s fine. (This is how I think of the fediverse). I can moderate my own server easily enough, and even with the Act in force, the moderation burden might not change all that much (depending on how it is implemented – see below).</p>

<p>If, however, the Act sees a large social media network that distributes the load across multiple resources, then it’s going to raise a great deal of problems for admins like myself. It’s going to create big burdens – and could even lead to more harmful speech.</p>

<h3 id="digital-safety-plan">Digital Safety Plan</h3>
<p>The issue with how the Act would see the fediverse is really intense when it comes to the Digital Safety Plan section. “The operator of a regulated service must submit a digital safety plan to the [Digital Safety] Commission in respect of each regulated service that it operates,” the Act states.</p>

<p>The details are going to matter a great deal here. As someone attempting to immigrate to Canada, I have some familiarity with Canadian bureaucracy. Much of it is quite straightforward, but some of it is onerous and confusing. If AoIR.social will have to supply such a plan, what will it entail? How will it be submitted? Will it be like getting my social insurance number (which was super easy) or like going through the work permit process (which was pretty labor-intensive)?</p>

<p>Looking at the details, I have concerns. For example is point G of the Digital Safety Plan, which would ask me to report</p>
<blockquote>
  <p>information respecting (i) the number of times that content that was accessible on the service was flagged to the operator by users of the service as being harmful content, including the number of flags relating to each type of harmful content, (ii) the manner in which the operator triaged and assessed the flags, (iii) measures taken by the operator with respect to content that was flagged as being harmful content, and (iv) the time within which the operator took measures with respect to content that was flagged as being harmful content.</p>
</blockquote>

<p>This would be easy enough – again, <em>if it’s limited to my server.</em> AoIR.social members are a collegial, professional lot. I simply don’t expect any of them to post harmful content, and thus I don’t expect to have to often report that they flagged content on our own server. And maybe this would be the case. I can indeed interpret the Act’s “users of the service” to be limited to AoIR.social members.</p>

<p>But this does raise a separate potential problem. If the Act sees the fediverse as a bunch of independent servers, what happens when someone outside my server flags something on my server? For the sake of argument, let’s say AoIR.social is actually a bad actor on the fediverse, and our members share harmful content regularly, and that we don’t report each other (since we all agree with our harmful content-sharing project). Could we simply ignore the pleas of people outside our server to cut it out? Could our Digital Safety Plan simply say, “we didn’t get reports”? The Act has penalties for such things, but it would require someone outside our server to report us to the Commission who would then have to investigate. Meanwhile, we’re gleefully sharing harmful content (in this hypothetical example).</p>

<p>So, maybe the Act should require me (and other fedi instances) to document flags I get from outside my server – that is, maybe the Act <em>should</em> see the fediverse as one large network.</p>

<p>But if the Act requires me to document flags coming from outside my server, I could also imagine another problem: being burdened by reporting on malicious or coordinated flagging from outside one’s server. I would have to write up reports for those flags, even if they are done in bad faith. There is language about my being able to ignore “vexatious” or “frivolous” flags, but do I have document them, anyway, to prove that all the reports were, indeed, vexatious?</p>

<p>Taken as a whole, I think the Act should treat each instance as an independent entity.</p>

<h3 id="duty-to-make-certain-content-inaccessible">Duty to Make Certain Content Inaccessible</h3>
<p>Subsection 67 of the Act says that if an operator finds material that exploits children or revictimizes an adult, or finds nonconsensual materials (e.g., revenge “porn”), the operator has to block it and tell the person that it’s blocked. This has to be done in 24 hours.</p>

<p>Obviously, I 100% support this. AoIR.social would act with the swiftness if a member posts such material.</p>

<p>But again, the details are going to matter, and the way the Act understands the fediverse is going to be important. If I block content from another server – not my own – do I have to inform the creator of the content, who did so on the other server? Because that sort of thing often leads to harassment of moderators. I or one of the AoIR.social moderators might get targeted by the sort of person who would maliciously share such content.</p>

<p>The next point, subsection 68, discusses “trivial, frivolous, vexatious” or “bad faith” flags, stating operators can ignore those.</p>

<p>That’s good.</p>

<p>But: “If the operator dismisses the flag, it must, within the period that applies under subsection (5), notify the user who flagged the content of the dismissal.” Again, this is pretty problematic if it’s someone outside my server. The sort of person who submits trivial or vexatious reports is not the sort of person who is going to simply walk away when an admin or mod says to stop.</p>

<h3 id="the-appeals-process">The Appeals Process</h3>

<p>The question of whether I am responsible for reaching out to people on other servers or not is intensified by the “Representations” section (subsection 69), which describes a sort of appeals process:</p>

<blockquote>
  <p>“If the operator of a regulated service makes content inaccessible under subsection 67(1) or 68(3), the operator must, within the period provided for by regulations, give the user who communicated the content on the service and the user who flagged the content, if applicable, an opportunity to make representations as to whether the content is content that sexually victimizes a child or revictimizes a survivor or intimate content communicated without consent.”</p>
</blockquote>

<p>If I’m required to communicate with someone sharing CSAM on another server and ask them to justify their doing so (representing that it is actually OK to share), I will definitely be burdened. I simply want to block them. Or more likely block their server for engaging in poor moderation. I don’t want to negotiate the finer points of CSAM with people who share it.</p>

<p>This is compounded by the “Reconsideration Upon Request” (point 70): “At the request of the user who communicated the content on the service or the user who flagged the content, the operator of a regulated service must reconsider a decision that it made under subsection 69(2).” If I am asked to do this work with someone outside my server, then a full process could be:</p>

<ul>
  <li>consider it and judge it</li>
  <li>communicate with the CSAM provider (“Hey, is this really CSAM?”)</li>
  <li>Hear them out</li>
  <li>Decide</li>
  <li>Reconsider upon request</li>
  <li>Decide</li>
  <li>Reconsider again (“Reconsideration” is mentioned twice)</li>
</ul>

<p>This strikes me as pretty burdensome, as well as a vector for harassment.</p>

<p>And this is not just if the Act sees the fedi as one big network. It could lead to harassment if the Act sees fedi instances as independent, too.</p>

<p>Here’s how: the Act includes the appeals process, because if the Act requires a company like Meta to block stuff without any chance of appeal, that’s problematic, since Meta is going to use automated moderation and will make tons of mistakes.</p>

<p>But for small fediverse instances, I don’t really think a government-mandated appeals process is necessary, nor wanted. Most well-moderated fedi instances are just going to remove accounts of people who post harmful stuff. No appeals. No tolerance. They’re not courts of law. The approach many fedi admins take is: if you want to post X content, you’re free to do so on another server. Does the Act give a bad actor the right to appeal the decision of a fedi admin? I really, really hope not. It would be burdensome and actually cause more harm than good.</p>

<h3 id="duty-to-keep-records">Duty to Keep Records</h3>
<p>Finally, let’s consider subsection 72: “The operator of a regulated service must keep all records, including information and data, that are necessary to determine whether the operator is complying with the operator’s duties under this Act.”</p>

<p>Depending on how the Act is implemented, this could be quite burdensome to operators of small servers. <em>Even if</em> the Act sees fedi instances as independent, I don’t really see the benefits of my keeping records on all my decisions. Do I need to record the date and time every time I block an account or outside instance? Do I need to screenshot every time someone flags something?</p>

<h2 id="conclusion">Conclusion</h2>

<p>In my view, I completely understand the rationale behind the Online Harms Act: I believe corporate social media ought to be regulated. However, I am concerned that, in writing these regulations, the Act’s authors may place some undue burdens on the emerging fediverse, which is comprised of tens of thousands of small servers around the world. If the Act’s authors understand fediverse servers to be independent entities, then the Act <em>might</em> not be so burdensome. If the Act sees the fediverse itself as a large social media network, problems compound. I’m not a lawyer but I believe that the Act is more likely to have the former interpretation.</p>

<p>But even if so, the Act could still burden small, self-funded social media sites by asking already overworked moderators and admins to do more work (or hire/recruit someone specifically to deal with these regulations). If so, the Act’s burdens could happen at a bad time, because the fediverse is starting to emerge as a viable alternative to corporate social media – the very thing the Act is no doubt attempting to address.</p>]]></content><author><name>RWG</name></author><category term="Online Harms Act" /><category term="Mastodon" /><category term="Fediverse" /><summary type="html"><![CDATA[The Online Harms Act is currently the talk of Canada. As the government’s website describes it, The internet is an exceptional tool for people of all ages to learn, play and connect with family, friends and those with similar interests. However, just like the outside world, the digital world can pose significant dangers. Social media can be used to sexually exploit children, promote self-harm to children, incite violence, put people’s safety at risk and foment hate. Online harms have real-world impacts with tragic, even fatal, consequences. The Government of Canada has introduced legislation to hold social media platforms accountable for addressing harmful content on their platforms and for creating a safer online space that protects all people in Canada, especially kids. As something that addresses speech on the internet – and includes some strong penalties for spreading hate speech – the Act has led to massive debates. I won’t summarize them here, but you can read thoughtful critiques from Professor Michael Geist and you can listen to a discussion on Canadaland’s Backbench to get a sense of things. My interest here is not to delve into the controversies, but instead read the Act while wearing my Mastodon admin hat. I am one of the two admins of AoIR.social, a Mastodon instance for members of the Association of Internet Researchers. AoIR.social gives our members access to the fediverse – the global network of thousands of social media servers, with millions of users engaged in social media activities. An initial question: does the Act apply to the tens of thousands of fediverse instances with their millions of users? Does it thus apply to AoIR.social? Considering that the Act defines “social media service” as “a website or application that is accessible in Canada, the primary purpose of which is to facilitate interprovincial or international online communication among users of the website or application by enabling them to access and share content” (page 5), then the answer is yes. AoIR.social operates internationally, including in Canada. So do many of the other tens of thousands of fediverse servers. Since I’m one of the two admins of AoIR.social, am I affected? I believe so: I would be an “operator,” “a person that, through any means, operates a regulated service.” It could also be that the AoIR organization itself would be the operator, since in this Act “person” also applies to corporations. The Act would create a Digital Safety Commission, an agency that would govern social media services. The Act notes this Commission should consider things like the size of the service and the operator’s financial and technical abilities. This is good – many fediverse instances are small and are run by individuals or a handful of folks. They also often run on very small budgets as not-for-profit organizations. Regardless, however, I believe small fediverse servers would be affected (unless they find a way to not be available to Canadians). So: will the Act radically affect fediverse instances, such as AoIR.social? The Act specifically says our service should not harm freedom of expression, and reading it leads me to believe that AoIR.social won’t be affected in that manner. While we welcome debate and discussion, we’re not in the business of platforming the sorts of harmful content the Act discusses. But, the implementation of the Act could prove to be very burdensome to the administration of fediverse instances, such as AoIR.social. Much is going to hinge on how the Act understands the fediverse.]]></summary></entry><entry><title type="html">Researching the Fediverse: Instances and Individuals</title><link href="http://localhost:4000/2024/03/14/Instances-and-Individuals.html" rel="alternate" type="text/html" title="Researching the Fediverse: Instances and Individuals" /><published>2024-03-14T00:00:00-04:00</published><updated>2024-03-14T00:00:00-04:00</updated><id>http://localhost:4000/2024/03/14/Instances%20and%20Individuals</id><content type="html" xml:base="http://localhost:4000/2024/03/14/Instances-and-Individuals.html"><![CDATA[<p>I just read a new article focusing on Mastodon: Christina Dunbar-Hester’s “<a href="https://firstmonday.org/ojs/index.php/fm/article/view/13367/11436">Showing Your Ass on Mastodon</a>.” (I’ve put in <a href="/bib.html">the bibliography</a>, too). It has inspired a blog post!</p>

<p>Dunbar-Hester’s article discusses a controversy – and a harassment campaign – centering on the #asstodon hashtag on the fediverse. She created (or co-created) that hashtag in late 2022, not long after the large wave of people joined Mastodon due to Musk’s purchase of Twitter. The #asstodon hashtag was used predominantly to share pictures of donkeys. However, because of the double-meaning, there was at least one buttshot included in the mix.</p>

<p>In this paper, which is a kind of accidental autoethnography, Dunbar-Hester discusses how one donkeyposter become irate over the presence of human butts instead of donkeys in #asstodon posts. The donkeyposter eventually began harassing people – including Dunbar-Hester, using multiple accounts, posting racist content, and even emailing Dunbar-Hester at her work.</p>

<p>In researching donkeyposter’s actions, Dunbar-Hester argues that the structure of Mastodon may not be amenable to hashtag activism.</p>

<p>I should note the article has a delightful amount of butt puns! I only managed one in this post (see if you can find it).</p>

<p>I want to draw on Dunbar-Hester’s article to write about something I’ve been wrestling with: methodologies for studying the fediverse. Specifically, I want to highlight a distinction between Dunbar-Hester’s approach and the one I’ve chosen for my <a href="/2024/02/11/Move-Slowy-Preview.html">book project</a>. This is not to say Dunbar-Hester did it wrong – in fact, her paper is excellent, and as fediverse scholarship grows, we will need multiple studies and many perspectives. I will highlight my instance-centric approach as a contrast.</p>

<!-- more -->

<h2 id="the-individual-member-perspective">The Individual Member Perspective</h2>
<p>Dunbar-Hester notes multiple times she did not start out to do research on Mastodon (let alone the broader fediverse). Instead, like many folks, she came there after Musk bought Twitter. Like other academics, she worked to rebuild networks on Mastodon, using a hashtag (#commodon) to gather.</p>

<p>However, she gained a great deal of insight into Mastodon’s socio-technical politics when she started using/creating a whimsical #asstodon hashtag. The goal was to inject a bit of levity into the otherwise professional media studies community; she posted #asstodon with a picture of a donkey, and soon others followed suit. This replicates what other animal lovers have done – #dogsofmastodon is one of my favorite hashtags. And, sure, there are things for cats – even though cats are terrible pets.</p>

<p>Like many hashtags, this one somewhat escaped her control. In my book, I write about how #fediblock – a hashtag created by two women – was appropriated without credit by people building blocklists. The women, Marcia X and Ginger, have since gotten credit, but only after asserting their role in creating the hashtag.</p>

<p>In the case of #asstodon, the donkeyposter saw it as a chance to create “donkey content” on the network, sharing pictures of (their own? it’s not entirely clear) donkeys. The donkeyposter seems to have claimed the hashtag for their own, even going so far as to ask people to only use it for donkeys and not, inevitable as it was, human derrières. It seems that people did not listen to donkeyposter, who then began harassing folks for even suggesting that “ass” has multiple meanings. Dunbar-Hester came into donkeyposter’s sights when she agreed that “a thousand asses should bloom.”</p>

<p>In trying to get to the bottom of it all, Dunbar-Hester and some allies realized that there is a kind of <em>Rashomon</em>-esque quality to studying the fediverse. Trying to find out what set off the donkeyposter on a noncentralized network was difficult: posts were deleted, admins (rightfully) blocked the donkeyposter after that person degraded into racism and harassment, and the view from one instance is not the same as the view from another. Dunbar-Hester uses this insight to argue that hashtag activism will not function the same on Mastodon as it did on Twitter, because the latter, for all its faults, could centralized all hashtags and give people a more global view of them. Mastodon and the fediverse, in contrast, can only give a partial view – a “lossy” one, to use her term.</p>

<p>While she did not start out wanting to research Mastodon, there are few people more qualified to do this work: Dunbar-Hester is the author of two excellent books: <em><a href="https://mitpress.mit.edu/9780262534765/low-power-to-the-people/">Low Power to the People</a></em> focuses on community radio, and <em><a href="https://press.princeton.edu/books/hardcover/9780691182070/hacking-diversity">Hacking Diversity</a></em> focuses on diversity advocacy in FOSS communities. Mashing the two together – community media and FOSS – and you get tremendous insight into Mastodon and the fediverse. So when she flips the switch and investigates “donkeygate” on the fedi, she is well-equiped.</p>

<p>But, as she notes, she only makes claims from her own perspective. It’s the perspective of an individual Mastodon member – a valuable one, but a limited view.</p>

<h2 id="the-instance-perspective">The Instance Perspective</h2>
<p>Unlike Dunbar-Hester, I’ve explicitly set out to research Mastodon and the fediverse from day 1.</p>

<p>I’ve focused on a particular aspect of the fediverse: instance-to-instance relations. I’ve highlighted what I call (drawing on a <a href="https://www.tandfonline.com/doi/full/10.1080/1369118X.2022.2147400">paper co-authored with Diana Zulli</a>) the “covenantal” approach to network building, where the smallest unit is the group. I focus on the decisions to maintain or break federated connections, which means I spend a great deal of time analyzing the role of codes of conduct, blocklists, funding models, and hashtag coordination in building a network of instances.</p>

<p>I chose this because the fediverse is comprised of instances, and the act of making an instance is a political act – an instance admin must set policies, decide who should be allowed to sign up, and how long connections between instances should be maintained. Even if an admin shirks these things and says “anything goes,” that’s still a political decision.</p>

<p>And choosing an instance does require some effort – selecting one, agreeing to a code of conduct, and learning the ropes of a specific community. Even if someone chooses an instance at random, there’s still some demand that the person abide by norms. This is why I tend to use the term “member” instead of “user” for individual fediverse accounts.</p>

<p>I do focus on individual members in the book, particularly artists, mutual aid practitioners, and people who have shifted from Twitter. But by and large my view tends to put these individuals in relation with instances or larger instance-to-instance relations.</p>

<p>Overall, I privilege admins and moderators in my research. And this is a limitation of my own perspective. While I can see much from this view, it – like Dunbar-Hester’s – is not a view from nowhere. It is situated, as well.</p>

<h2 id="multiple-perspectives-needed">Multiple Perspectives Needed</h2>
<p>I contrast our approaches because they reveal ways forward for other researchers. Echoing Dunbar-Hester, I would also say: someone ought to write this up! More research on the fediverse is needed. My approach is not superior to Dunbar-Hester’s. I can see certain things, and she can see others. So, future researchers, be mindful of perspective.</p>

<p>Her approach provides valuable documentation and insight into what it’s like to be a skilled researcher coming to the fediverse. The individual, self-reflexive perspective Dunbar-Hester brings is incredibly important. It can inform work on human-computer interaction, masspersonal communication, and activism, among other things.</p>

<p>But as Dunbar-Hester found, it has limits: individuals can only get a partial view. In some ways, this is a good thing – I would be nervous if any one person could get a global view of the fediverse without the consent of network members. I don’t even think it’s wise for <em>researchers</em> to get a global view without the consent of members. At most, I would support a global view of instances, but not the ability to dive into any given individual account.</p>

<p>More importantly, if we stress individual perspectives, we run the risk of erasing instances. If the fediverse is a network of individuals who can interact without regard to the instance, than instance administration becomes a mere service and not a political act. (In some ways, I think Bluesky is going this direction, and I fear that such an approach replicates the eliding of things like moderation, much as corporate social media has done, hiding moderation practices behind layers of abstraction).</p>

<p>So what are the limitations of my more instance-centric view? Well, obviously, the perspective of individual members can get lost.</p>

<p>But more importantly, it puts a lot of weight on the admin’s perspective – and as <a href="https://www.ucpress.edu/book/9780520393943/governable-spaces">Nathan Schneider has argued</a>, this (crypto or explicit) valorization of the almighty sysadmin can lead to authoritarianism. The danger is that an instance becomes a pseudo-state, with the admin reigning supreme. Privileging the admin may not challenge this view. (I agree, to a point, but I would note that noncentralized instance-to-instance relations, as well as the ability to move instances, mitigate the power of individual admins to a large degree).</p>

<p>Ultimately, of course, Dunbar-Hester’s work doesn’t merely provide a foil for what I’m doing. Her conclusion is that activists need to influence the fediverse for more tools to make large-scale collective organizing possible. I agree. What those tools end up looking like – individual- or instance-centric – will shape the network to come, but in any case, the status quo may not be working well for activists.</p>]]></content><author><name>RWG</name></author><category term="Goal 2" /><summary type="html"><![CDATA[I just read a new article focusing on Mastodon: Christina Dunbar-Hester’s “Showing Your Ass on Mastodon.” (I’ve put in the bibliography, too). It has inspired a blog post! Dunbar-Hester’s article discusses a controversy – and a harassment campaign – centering on the #asstodon hashtag on the fediverse. She created (or co-created) that hashtag in late 2022, not long after the large wave of people joined Mastodon due to Musk’s purchase of Twitter. The #asstodon hashtag was used predominantly to share pictures of donkeys. However, because of the double-meaning, there was at least one buttshot included in the mix. In this paper, which is a kind of accidental autoethnography, Dunbar-Hester discusses how one donkeyposter become irate over the presence of human butts instead of donkeys in #asstodon posts. The donkeyposter eventually began harassing people – including Dunbar-Hester, using multiple accounts, posting racist content, and even emailing Dunbar-Hester at her work. In researching donkeyposter’s actions, Dunbar-Hester argues that the structure of Mastodon may not be amenable to hashtag activism. I should note the article has a delightful amount of butt puns! I only managed one in this post (see if you can find it). I want to draw on Dunbar-Hester’s article to write about something I’ve been wrestling with: methodologies for studying the fediverse. Specifically, I want to highlight a distinction between Dunbar-Hester’s approach and the one I’ve chosen for my book project. This is not to say Dunbar-Hester did it wrong – in fact, her paper is excellent, and as fediverse scholarship grows, we will need multiple studies and many perspectives. I will highlight my instance-centric approach as a contrast.]]></summary></entry><entry><title type="html">“Move Slowly and Build Bridges” Preview</title><link href="http://localhost:4000/2024/02/11/Move-Slowy-Preview.html" rel="alternate" type="text/html" title="“Move Slowly and Build Bridges” Preview" /><published>2024-02-11T00:00:00-05:00</published><updated>2024-02-11T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/11/Move%20Slowy%20Preview</id><content type="html" xml:base="http://localhost:4000/2024/02/11/Move-Slowy-Preview.html"><![CDATA[<p>So… it’s coming together.</p>

<p>My current book, <em>Move Slowly and Build Bridges: Mastodon, the Fediverse, and the Struggle for Ethical Social Media</em> is mostly drafted! I am drafting a conclusion, but for now, I figured I would provide an overview of the book.</p>

<p>This post gives a chapter-by-chapter summary of the draft. Since it’s a draft, things can still change, but as of today, I’m pretty pleased with the structure.</p>

<!-- more -->

<h2 id="introduction">Introduction</h2>
<p>The introduction gives a bit of background on the fediverse and justifies a focus on Mastodon. I also discuss the history of alternative social media – activists have been trying help us move past corporate social media for a long time. I define terms used throughout the book, like “noncentralization” and “covenantal fediverse.”</p>

<h2 id="chapter-1-from-techlash-to-alternative-or-how-to-vaporize-elon-musk">Chapter 1: From Techlash to Alternative, or How to Vaporize Elon Musk</h2>
<p>This chapter answers a key question: what pushes people to leave corporate social media? We’ll look at the recent purchase of Twitter by Elon Musk, which prompted millions of people to leave Twitter and switch to Mastodon and the broader fediverse. What does it look like to flee corporate social media for a noncentralized alternative? And, since Mastodon instances are often run by volunteers, and what does it look like to deal with a wave of new members? And what does it mean to offer an “alternative”? Drawing on interviews with many people affected by Musk, the chapter contrasts the actions of a single billionaire with communities of people helping each other.</p>

<h2 id="chapter-2-the-non-standard-standard-when-activitypub-met-mastodon">Chapter 2: The Non-Standard Standard: When ActivityPub met Mastodon</h2>
<p>Chapter 2 focuses on the technical achievement of noncentralization. Based on interviews, it tells the  a key protocol, ActivityPub, and the queer and trans developers who created it in spite of a series of obstacles. It turns out that the ActivityPub authors were creating something desperately needed by a then-unknown project called Mastodon, which was trying to find a way to provide protections for marginalized members in an internet culture that demanded total transparency. In turn, Mastodon provided a boost to the creators of ActivityPub by using a beta version of the protocol. (I talk about an early version of this chapter in a <a href="/2023/10/15/APnonStandard.html">previous blog post</a>.)</p>

<h2 id="chapter-3-codes-of-conduct">Chapter 3: Codes of Conduct</h2>
<p>This chapter discusses the social contract shared among many Mastodon moderators. Mastodon is a system comprised of many small servers. You could take its code and install it on your computer, but thanks to ActivityPub, your computer can talk to other Mastodon computers around the world. With all these different Mastodon servers running, how is social coordination possible? The answer is codes of conduct. Championed by trans technologist Coraline Ada Ehmke (interviewed for this project) in the mid-2010s, codes of conduct add a social layer to networking technologies. With codes of conduct, Mastodon moderators can make decisions about connecting to one another while promoting the safety of their communities. In addition, codes of conduct are not concocted by corporate lawyers, but are democratically made by the community, for the community. Thus, Mastodon and the fediverse is more than just a technical achievement – it’s a social one, as well.</p>

<h2 id="chapter-4-rage-and-joy-playvicious-fediblock-the-badspace-and-the-politics-of-defederation">Chapter 4: Rage and Joy: Playvicious, #Fediblock, the BadSpace, and the Politics of Defederation</h2>
<p>This one builds on the previous chapter by telling the story of how Mastodon and the fediverse decide when to block, rather than connect, to another server. What happens when an entire portion of the network trolls, harasses, and misinforms the rest? The recourse Mastodon members have is to block them. But making the decision to block a server is not a straightforward one. The story can be told through the experiences of Black fediverse members, such as Ro and Marcia X. Ro is currently building The Bad Space, a mechanism to easily share blocklists across the fediverse. His work builds on that of his friend, Black feminist Marcia X, who has engaged in hashtag activism, making the now ubiquitous #fediblock hashtag. Both The Bad Space and #fediblock have been criticized as inimical to federation, but the experiences of Black Mastodon members shows us that we need to have these tools – and that we have to do the hard work of collectively making blocking decisions – if we want to have a thriving and anti-racist fediverse.</p>

<h2 id="chapter-5-paying-for-it-the-fediverses-alternative-economies">Chapter 5: Paying for It: The Fediverse’s Alternative Economies</h2>
<p>Chapter 5 addresses a question I always get when I talk about the fediverse: who pays for it? After 20 years of corporate social media and surveillance capitalism, we have forgotten that there are other economic models for digital media. The chapter explores the non-capitalist elements of the fediverse, including the use of donations and non-profit organizations to fund servers, mutual aid between fediverse members, and how artists and musicians use the network to promote their work. Together, these folks are forging an alternative not just to corporate social media, but to capitalism itself.</p>

<h2 id="chapter-6-to-finity-and-before-degrowth-solarpunk-and-environmentalist-experiments-on-the-fediverse">Chapter 6: To Finity and Before: Degrowth, Solarpunk, and Environmentalist Experiments on the Fediverse</h2>
<p>This chapter focuses on the ecological impact of social media. While we’re encouraged to think of corporate social media as being a slick, clean, even immaterial system, in reality data storage and processing take a tremendous toll on resources. If the fediverse is to be a true alternative, we have to rethink how social media affects the environment. This chapter introduces us to degrowth thinkers and solarpunks who are using fediverse technologies to mitigate the climate disaster.</p>

<h2 id="chapter-7-threads">Chapter 7: Threads</h2>
<p>This one considers the corporate reaction to the fediverse. The fediverse is expanding, and thus it’s attracting the attention of corporate social media, which has traditionally simply bought out the competition, or had it regulated away. The latest to try is Meta, the owner of Facebook, Instagram, and WhatsApp. In early 2023, Meta announced that their latest project, a Twitter-like system called “Threads,” would adopt ActivityPub and thus be able to federate with Mastodon and fediverse instances. This chapter focuses on the #fedipact, a campaign started by a trans woman by the name of vanta black who is leading a  resistance to Meta’s effort to federate.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This is the part I’m still drafting. I’m taking the “ethical” seriously in the subtitle and thinking through ethical theories and the fediverse. In addition to being inspired by the great people I’ve met doing the research for the book, this part was also inspired by <a href="/2023/11/22/EthicsAndCOCs.html">a class exercise I did with students</a> in my recent Media Ethics class. More to come about this chapter…</p>

<h2 id="lets-talk">Let’s Talk!</h2>
<p>If you like what you see here, let’s talk! I’m looking to talk about this project with podcasters and journalists, not to mention students and other researchers. The biggest thing I’ve learned about the fediverse is that people can, in fact, change media for the better. It’s a struggle, but it can be done. I want to share this with others, and also I hope this work inspires more academic and journalistic engagement with the fediverse. To paraphrase Captain Picard, I think we’re only at the beginning of the adventure.</p>]]></content><author><name>RWG</name></author><category term="Goal 2" /><category term="Move Slowly and Build Bridges" /><summary type="html"><![CDATA[So… it’s coming together. My current book, Move Slowly and Build Bridges: Mastodon, the Fediverse, and the Struggle for Ethical Social Media is mostly drafted! I am drafting a conclusion, but for now, I figured I would provide an overview of the book. This post gives a chapter-by-chapter summary of the draft. Since it’s a draft, things can still change, but as of today, I’m pretty pleased with the structure.]]></summary></entry><entry><title type="html">Thoughts on Threads, or Is Mark Zuckerberg Jesus?</title><link href="http://localhost:4000/2023/12/16/ThoughtsThreads.html" rel="alternate" type="text/html" title="Thoughts on Threads, or Is Mark Zuckerberg Jesus?" /><published>2023-12-16T00:00:00-05:00</published><updated>2023-12-16T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/16/ThoughtsThreads</id><content type="html" xml:base="http://localhost:4000/2023/12/16/ThoughtsThreads.html"><![CDATA[<p>(Sorry about the provocative title, but I just had to do it.)</p>

<p>Today, a lot of people have thoughts on Threads, which is <a href="https://techcrunch.com/2023/12/14/mastodon-founder-touts-threads-federation-saying-it-makes-his-x-rival-a-far-more-attractive-option/">actively testing ActivityPub federation</a>. The reactions range from triumphant proclamations about this making the fediverse more legitimate to abject horror that a company so reviled is attempting to join the fedi.</p>

<p>Full disclosure: I’m more in the latter camp.</p>

<p>As readers of this blog know, I’m working on a <a href="/2023/08/17/OxfordUP.html">book about the fediverse</a>, and part of my process is to share ideas out in the open. Since any book about the fediverse will have to talk about Threads, I figured this might be a good place to share a rough idea of how a chapter about Threads might go. Comments are welcome, of course!</p>

<p>Away we go…</p>

<!-- more -->

<h2 id="recapitulation">Recapitulation</h2>
<p>There’s a term I heard a few months back, a bit by accident: “recapitulation.”<a href="#note1">[1]</a> I’ve been been rolling it around in my mind, like a marble in the palm of the hand. It’s so generative.</p>

<p>And today, as Meta’s Threads starts the long-awaited (or long-feared) process of federating via ActivityPub, I think I have a way to pick apart what’s going on with this term.</p>

<p>After all, “recapitulation” is a kind of keyword in the sense <a href="https://en.wikipedia.org/wiki/Keywords:_A_Vocabulary_of_Culture_and_Society">Raymond Williams meant</a>: a term that has multiple, contested meanings, meanings that are bound up with multiple problems. Look it up in the Oxford English Dictionary and you’ll find three ways “recapitulation” helps us pick apart Meta’s Threads and the fediverse:</p>
<ul>
  <li>a Christian theological meaning, where all of human history is recapitulated by Jesus Christ,</li>
  <li>a summing up of previous arguments, and</li>
  <li>a unificiation.</li>
</ul>

<p>Let’s take a look at each in turn.</p>

<h2 id="theological-recapitulation">Theological recapitulation</h2>
<p>According to the Oxford English Dictionary, the theological meaning of “recapitulation” dates back at least to 1629: “the summing up and redemption of all human experience in the life and death of Christ.” In this <a href="https://www.pharosjot.com/uploads/7/1/6/3/7163688/article_1_vol_101__2020_.pdf">theological theory</a>, Jesus Christ is a second Adam, one who did not sin. In delivering the rest of us from sin, human history from the Fall of Adam to the present is summed up under the name of Christ. Our acceptance of Christ allows for a return to pre-lapsarian life.</p>

<p>Ok, what the hell does that have to with Threads?</p>

<p>Well, let me drop a few quotes from coverage of Threads:</p>

<ul>
  <li>Kalhan Rosenblatt, <a href="https://www.nbcnews.com/tech/threads-meta-twitter-rival-app-reaction-nostalgia-chaos-rcna92867">writing for NBC</a>: “Threads has been chaotic yet blissful, users said, noting the app reminded them of the early days of the internet — especially since ads have yet to hit the platform.”</li>
  <li>Tamara Palmer, <a href="https://48hills.org/2023/07/threads-vs-twitter-look-ma-no-nazis-yet/">writing in 48Hills</a>: “Threads has a much lighter and nostalgic feel than Twitter at the moment; I even got to, um, Thread with a Vanderpump Rules star last night. (They’re just like us!) It almost feels like 2006 again.”</li>
  <li>Jay Clouse, <a href="https://creatorscience.com/">writing in Creator Science</a>: “For the last several days, I’ve actually published more on Threads than anywhere else. And not because I’m chasing an opportunity – because it was more fun for me!”</li>
  <li>Kari Paul, <a href="https://www.theguardian.com/technology/2023/jul/06/we-tried-threads-metas-new-twitter-rival-heres-what-happened">writing in the Guardian</a>: “As a tech writer who has reported extensively on the privacy concerns surrounding Meta, the company’s shameless copying of competitors’ apps, and tech’s growing unchecked power, it pains me to say that I actually enjoyed using Threads.”</li>
</ul>

<p>There’s a clear theme here: nostalgia for a more fun time online. It’s a time before something awful ruined it all. The nostalgic tone becomes even clearer when we consider the context: Threads started not long after Elon Musk purchased Twitter and turned it into a hellscape.</p>

<p>Across much of the coverage of Twitter alternatives (including Bluesky and, to be fair, Mastodon), there’s a sense that the new thing is a return to a better, lost time. For some, it was Twitter before #gamergate. For others – I think journalists fall in this camp – it’s a time when they had authority and were respected. For others, it’s Twitter before Donald Trump. And for everyone, it’s Twitter before Musk.</p>

<p>Astute readers will see where I’m going with this: Mark Zuckerberg is here to cleanse us of our sins and take us back to the Garden of Microblogging Eden. Or I should put it like this: Meta, a major social media corporation, is swooping in to make microblogging great again.</p>

<p>Zuckerberg – or maybe Meta – is Jesus.</p>

<p>(I apologize if those last few sentences made you want to smash your phone or computer against the wall.)</p>

<p>As Meta federates via ActivityPub, this sort of nostalgia might get amplified. If we could only return to Twitter before the Fall, many people seem to be saying. Who will lead us there?</p>

<h2 id="repetition-of-the-argument">Repetition of the argument</h2>
<p>So that’s one meaning of recapitulation. But of course, it’s not the only one, and probably not the first one most readers thought of.</p>

<p>The most common meaning is (again, per the OED) “The action or an act of recapitulating (something); a brief restatement or repetition; a summing up; a summary.” This is where we get the term “recap,” a restatement, a summary form of an argument or plot. (It’s also used in music in sonatas, – the repetition of a melody). The verb form is “to go through or repeat again, usually in a more concise manner; to go over the main points or substance of (an argument, statement, etc.); to summarize, restate briefly.”</p>

<p>When it comes to Meta and the fediverse, oh, yeah – there are a lot of repeated arguments happening.</p>

<p>As <a href="https://www.youtube.com/watch?v=yZoASOyfvGQ">Derek Caelin has documented</a>, the fediverse went through a massive debate when another large entity attempted to federate. That entity was Gab.com, a white Christian nationalist social networking site. Or, if you prefer, a Nazi site, since it does host self-described Nazis.</p>

<p>It should have been easy to block Gab. However, while the story told about Gab today is that it was defederated quickly, people who lived through that period note how much benefit of the doubt Gab got. The counter-argument against blocking Gab was pretty simple: openness and free speech. That is, rather than preemptively blocking them, this argument goes, we should give them a chance. Ours is an open network. When they say racist stuff, we should counter it with rational, anti-racist arguments. If any of their users are truly bad actors, we will eventually see evidence of it, and then we can block those users. If after a period of time it turns out that the whole domain is a bad actor, then block them.</p>

<p>Basically, let them join, see how they act, and then block. That sort of thing.</p>

<p>It’s a naive view of free speech that critical theorist Herbert Marcuse would call <a href="https://www.marcuse.org/herbert/publications/1960s/1965-repressive-tolerance-fulltext.html">“repressive tolerance.”</a> As Marcuse argues, if we tolerate dominant ideas (e.g., white supremacy, transphobia), then we effectively eradicate marginalized ideas.</p>

<p>Those who argued for preemptively blocking Gab recognized this. Rather than allowing for self-described Nazis to spout hate, the Gab-blockers rejected the free speech and openness argument in favor of supporting marginalized people so <em>they</em> could have a chance to speak. Because spaces for the marginalized to speak are rare.</p>

<p>Those who called for preemptive blocking were called tyrants, because they would violate the sanctity of openness and free speech. The naive free speech view, while naive, is powerful in its simplicity.</p>

<p>Fast forward to today: we’re recapitulating that debate.</p>

<p>Back in June or so, vanta black created <a href="https://fedipact.online/">the Fedipact</a>, asking admins to preemptively block Threads (full disclosure, my instance is on the list). Why? Well, <a href="https://fedipact.online/why">I’ll just quote her</a>:</p>
<blockquote>
  <ul>
    <li>that time [Meta] helped facilitate a genocide</li>
    <li>that time they helped try to rig an election</li>
    <li>that time they did creepy behavioral experimentation on their users</li>
  </ul>
</blockquote>

<p>We could add more to vanta’s list: Meta platforms hate speech (Libs of Tiktok is often invoked here). Instagram has a destructive influence on teen girls’s sense of worth. Meta attempts to moderate globally on the cheap with underpaid laborers (who get crushed when they try to unionize for their rights).</p>

<p>And none of this is to mention the idea that prompted my whole academic career back in 2008: on Meta properties, we do the free work of declaring what we like, and then Meta sells our attention to advertisers.</p>

<p>So, blocking Meta’s Threads should be easy, right?</p>

<p>It turns out, not so much. Fedipact has received a lot of criticism. Other people have summed up (or made) the arguments in favor of not blocking Meta, so I won’t exhaustively recap them, but here are the big ones:</p>
<ul>
  <li>It will be good for ActivityPub,</li>
  <li>It will legitimate the fediverse,</li>
  <li>It will allow fediverse members to increase their follower counts, or</li>
  <li>of course: free speech and openness</li>
</ul>

<p>In spite of all the well-documented, terrible, awful, horrifying things that company has done, <em>we are recapitulating the same conflict we saw with Gab.</em> Just like Gab, Meta’s Threads is also getting a great deal of benefit of the doubt.</p>

<p>It’s just that this time, there feels like a lot more is at stake.</p>

<h2 id="unification">Unification</h2>
<p>What do we do about this? Where do we go from here?</p>

<p>Well, let’s look at a third (and admitedly obscure) meaning of “recapitulate”: “to come together into one.”</p>

<p>At first glance, this is the great fear that Meta’s Threads invokes. In my book, I’m arguing that Mastodon and the fediverse are a noncentralized system. Not <em>decentralized,</em> but <em>noncentralized</em>. By that I mean the fediverse is actively resistant to centers. For example, if an instance gets to be too big – Mastodon.social comes to mind – people start openly critiquing it, arguing folks should move accounts from it. So much of the structure of the fediverse, from its open protocols to its FOSS code to its cultural practice, is about noncentralization.</p>

<p>But with Meta in town? The fear is that this massive corporation will absorb the fediverse. It could do so in many ways: taking over the ActivityPub protocol. Drawing many of the most popular accounts away from the rest of the fedi and onto Threads. Overwhelming the fediverse with marketer and influencer content. Swamping the fedi with disinformation, conspiracy theories, and hate speech.</p>

<p>But maybe the third meaning of “recapitulation” shows us a way out.</p>

<p>What if the coming together into one, the recapitulation of the fediverse, is to unify – once again – against an unethical actor? I noted that the defederation of Gab wasn’t a simple thing, but it did happen.</p>

<p>Elsewhere, <a href="https://www.tandfonline.com/doi/full/10.1080/1369118X.2022.2147400">a colleague and I argued</a> that the emergent, shared ethical core of the fediverse represents a digital covenant. As instances develop their distinct codes of conduct, and as they federate, we see an emergent, shared, global-yet-thin set of deontological rules about content moderation.</p>

<p>The fediverse in 2023, I would argue, is an even stronger covenantal network than it was in the Gab days. Let’s look at what’s going on: Fedipact. #Fediblock. Communal blocklists. The Mastodon Server Covenant. There is a shared ethical core on the fediverse, and it is possible that this unity-through-federation could be successful in fending off Meta.</p>

<p>This is why the third meaning of recapitulation is my favorite. Because it’s not nostalgic or atavistic. It’s not about a sole figure (or a sole corporation) saving us. It’s not a rehash of an argument we endlessly have. It’s about unifying. And the fediverse could do the seemingly impossible: unifying through heterogeneity.</p>

<p>A noncentralized unity.</p>

<h2 id="notes">Notes</h2>

<p><a name="note1">[1]</a> My digging into the concept of recapitulation happened by accident. One of my interviewees used it to refer to how mainstream culture absorbs alternative practices. As a commenter on the fediverse noted, my interviewee most likely meant to say “recuperation”, a <a href="https://en.wikipedia.org/wiki/Recuperation_(politics)">term of art from the Situationists</a>. Based on the context of the interview, I would say this is the case. I’ve edited this post to reflect this to respect the fact that my interviewee probably didn’t intend me to interpret him that way.</p>

<p>I now consider this take on Threads to be the product of a happy accident, since looking at recapitulation led me to the idea of some savior coming to take us back to a lost time. I think the narrative around Threads outside of the fedi has been about nostalgia. I <em>definitely</em> think the narrative about Bluesky has been about nostalgia. Even the discourse about Mastodon in its early days was about that.</p>

<p>And I also like the idea of a unification-through-difference that the third meaning implies.</p>

<p>However, I don’t think this post will form the basis of the chapter of the book. Some ideas will travel from it – including some ideas based on the feedback I got on the fedi – but this recapitulation idea is a bit too convoluted to make it into my book, which is intended for a broad audience.</p>]]></content><author><name>RWG</name></author><category term="Meta" /><category term="Threads" /><category term="digital covenant" /><summary type="html"><![CDATA[(Sorry about the provocative title, but I just had to do it.) Today, a lot of people have thoughts on Threads, which is actively testing ActivityPub federation. The reactions range from triumphant proclamations about this making the fediverse more legitimate to abject horror that a company so reviled is attempting to join the fedi. Full disclosure: I’m more in the latter camp. As readers of this blog know, I’m working on a book about the fediverse, and part of my process is to share ideas out in the open. Since any book about the fediverse will have to talk about Threads, I figured this might be a good place to share a rough idea of how a chapter about Threads might go. Comments are welcome, of course! Away we go…]]></summary></entry><entry><title type="html">Ethics and Codes of Conduct on Mastodon</title><link href="http://localhost:4000/2023/11/22/EthicsAndCOCs.html" rel="alternate" type="text/html" title="Ethics and Codes of Conduct on Mastodon" /><published>2023-11-22T00:00:00-05:00</published><updated>2023-11-22T00:00:00-05:00</updated><id>http://localhost:4000/2023/11/22/EthicsAndCOCs</id><content type="html" xml:base="http://localhost:4000/2023/11/22/EthicsAndCOCs.html"><![CDATA[<p>Yesterday, I taught one of the final meetings of my undergrad “Ethics and the Media” course at York. We’re using <a href="https://bookshop.org/p/books/digital-media-ethics-charles-ess/7513541?ean=9781509533428">Charle Ess’s <em>Digital Media Ethics</em> (3rd edition)</a> as our textbook, and so we’ve had a good guide to ethical theories, such as virtue ethics, deontology, utilitarianism, and feminist ethics of care. (I also supplement Ess with readings and ideas from Shannon Vallor, Kwame Gyekye, Carissa Veliz, Michael Zimmer, and others).</p>

<p>Much of the course has been about learning the ethical theories and then discussing <a href="https://mediaengagement.org/vertical/media-ethics/">case studies</a>. It’s been quite an enjoyable course to teach, particularly because I’ve seen the students go from the vague and intellectually unsatisfying statement “Ethics is a subjective, personal thing” to the more robust “Let me lay out how a virtue ethicist would think about this.”</p>

<p>But I wanted to have at least one day of the course dedicated to applied ethics. Instead of looking at a case that happened in the past, I wanted the students to apply the ethical theories to a concrete situation.</p>

<p>And these days, we have a great situation in digital media ethics to consider: what to do when we start up a Mastodon server?</p>

<p>In this post, I’ll write up what we did in the class, in case anyone out there wants to try something similar.
<!-- more --></p>

<h2 id="mastodon-in-the-classroom">Mastodon in the classroom</h2>

<p>I spun up a relatively inexpensive Digital Ocean server and installed the latest version of Mastodon. I also imported a <a href="https://thebad.space">server blocklist</a>, because I did not want to have the students – all of whom were new to Mastodon – experience some nasty trolling in their first foray. At the same time, I still wanted students to be able to follow accounts outside of our server, so blocking known bad instances made the most sense. I recommend anyone using Mastodon in the classroom do something similar.</p>

<p>I then invited students to join and mess around with our server – post, share, make profiles, etc. I realized that they would see essentially an empty stream and few others – this is all part of the process. I wanted to stress to them that everything on the server is under our control, so we – not some big company – have to decide how active our server would be.</p>

<h2 id="what-about-a-code-of-conduct">What about a Code of Conduct?</h2>

<p>One step I skipped in setting up was setting server rules and a code of conduct. I did this on purpose: it was the point of the in-class discussion. After providing some background details about Mastodon and the fediverse and answering student questions, we then discussed codes of conduct. For background, I gave them a draft chapter of my <a href="/2023/08/17/OxfordUP.html">current book</a>, which focuses on the history of codes of conduct (and thus draws heavily on an interview with <a href="https://where.coraline.codes/">Coraline Ada Ehmke</a>).</p>

<p>After we talked codes of conduct, I talked about how running Mastodon requires us to be active in making choices about content moderation. Running a Mastodon server is a moment of applied ethics.</p>

<p>I then asked them to break into four groups:</p>

<ul>
  <li>One group focusing on <strong>feminist ethics of care,</strong></li>
  <li>Another focusing on <strong>deontology,</strong></li>
  <li>Another on <strong>virtue ethics,</strong></li>
  <li>And the last on <strong>utilitarianism.</strong></li>
</ul>

<p>I asked the group to first summarize the ethical theory (not an easy task, I know).</p>

<p>Next, I asked them to apply the ideas from their ethical theory to our potential Mastodon code of conduct.</p>

<p>I really, really liked the results!</p>

<p>Here are summaries of each:</p>

<h3 id="feminist-ethics-of-care">Feminist ethics of care</h3>
<p>Feminist ethics of care (FEOC) rejects the idea (most associated with deontology) that ethical decisions are the sole purview of individual, autonomous rational actors. Instead, FEOC argues we make ethical choices based upon, and informed by, relationships we have with one another. Moreover, an ethical choice is not purely a rational matter, but also involves emotions. Finally, FEOC elevates care of others to the center (but it rejects the idea that only one segment of our society is responsible for care work).</p>

<p>Based on this theory, the students argued that our Mastodon server code of conduct should</p>
<ul>
  <li>Describe how we can network with other servers who share our values.</li>
  <li>Take inspiration from the first clause of Canadian charter: your rights end where someone else’s begin. You can post so long as you don’t violate others’ rights.</li>
  <li>Emphasize contextual judgement (considering potential violations in context).</li>
  <li>Emphasize communication during moderation, using empathy for would-be violators.</li>
</ul>

<p>My take: this is a great set of insights. It echoes what I’ve learned over the past few years doing interviews with fediverse admins. Many told me that their approach to moderation is to talk to people and find out what’s going on. They also talk about how they choose to federate/block other instances.</p>

<h3 id="deontology">Deontology</h3>
<p>Deontology emphasizes individual rights and autonomy, as well as duties to do what is right. Intentions often matter more than consequences. Often deontology is expressed as rules (although that’s a bit reductive.)</p>

<p>Students in this group did offer rules, likely inspired by reading sample Mastodon codes of conduct:</p>
<ul>
  <li>Post nothing illegal!</li>
  <li>No discrimination based on age, race, gender, sexuality</li>
  <li>No harassment</li>
  <li>No trolling</li>
  <li>No defamation or coercion</li>
  <li>Show some respect in the community</li>
</ul>

<p>My take: if there’s one ethical theory that Mastodon seems designed for, it’s deontology. Mastodon’s software prompts the admin to put in “Server Rules,” short statements about expectations would-be members should live up to. Many, many Mastodon codes of conduct have rules similar to the above.</p>

<h3 id="virtue-ethics">Virtue Ethics</h3>
<p>Virtue ethics is less about intentions or consequences than it is about character. Virtues in a digital environment might include honesty, humility, courage, and compassion, and these are balanced against vicious extremes. Virtue ethics is also relational in that we teach children about virtues and provide advice to them when they have to make ethical choices.</p>

<p>The students in this group suggested that our code of conduct should</p>
<ul>
  <li>Call for displays of virtous traits, such as wisdom, courage, loyalty,</li>
  <li>Emphasize being polite and respectful, and</li>
  <li>Have high standards for moderation.</li>
</ul>

<p>My take: I was really curious what the students would say in this group, and they delivered some good stuff. What’s useful, I think, is contrasting the negative rules of deontology with these positive virtues and character traits. I could imagine a code of conduct that integrates virtue ethics to emphasize positive behaviors.</p>

<h3 id="utilitarianism">Utilitarianism</h3>
<p>Alongside another consequentialist theory, libertarianism, utilitarianism is probably the most familiar theory to me, since I grew up in the USA. (I’m not saying I endorse either; just that I think I soaked in them). Utilitarianism judges acts based on their consequences: if an act creates the greatest happiness for the greatest number of people, it’s an ethical one.</p>

<p>Students in this group emphasized the greatest happiness principle in their code of conduct ideas:</p>
<ul>
  <li>Let’s promote inclusivity to make people safe, welcome, and heard.</li>
  <li>We need to listen to user feedback.</li>
  <li>We need to consider well-being of the community.</li>
  <li>Server members should report stuff to the moderators.</li>
  <li>We should promote free speech but exclude hate speech.</li>
</ul>

<p>My take: Students really leaned into thinking about the would-be Mastodon server as a community, and so this approach emphasized group happiness. I think this was a good addition to the other ideas, because social media can be fun and happy – why not build a code of conduct that emphasizes these aspects?</p>

<h2 id="summing-it-all-up">Summing it all up</h2>

<p>We concluded the class by considering how these different approaches to a code of conduct could be synthesized. We kinda ran out of time for that, but I think we may have been able to write a pretty sweet code if we had more time.</p>

<p>I did spend some time talking about enforcement, since no code of conduct is worth anything if you’re not willing to live up to it. I ran various scenarios past them – what if someone outside our server violates our code? What if someone outside our server reports one of our members?</p>

<p>Their answers reflected the ideas I’ve heard from seasoned admins, including how admins discuss things with one another and how admins talk to members.</p>

<p>All in all, I think this experiment was really exciting. Unlike corporate social media – Facebook, TikTok, X, etc – <em>we</em> get to make decisions about moderation when we run our own instance. It’s a big responsibility, but these students showed they’re up for it.</p>]]></content><author><name>RWG</name></author><category term="Goal 2" /><category term="Mastodon" /><category term="Codes of Conduct" /><summary type="html"><![CDATA[Yesterday, I taught one of the final meetings of my undergrad “Ethics and the Media” course at York. We’re using Charle Ess’s Digital Media Ethics (3rd edition) as our textbook, and so we’ve had a good guide to ethical theories, such as virtue ethics, deontology, utilitarianism, and feminist ethics of care. (I also supplement Ess with readings and ideas from Shannon Vallor, Kwame Gyekye, Carissa Veliz, Michael Zimmer, and others). Much of the course has been about learning the ethical theories and then discussing case studies. It’s been quite an enjoyable course to teach, particularly because I’ve seen the students go from the vague and intellectually unsatisfying statement “Ethics is a subjective, personal thing” to the more robust “Let me lay out how a virtue ethicist would think about this.” But I wanted to have at least one day of the course dedicated to applied ethics. Instead of looking at a case that happened in the past, I wanted the students to apply the ethical theories to a concrete situation. And these days, we have a great situation in digital media ethics to consider: what to do when we start up a Mastodon server? In this post, I’ll write up what we did in the class, in case anyone out there wants to try something similar.]]></summary></entry></feed>